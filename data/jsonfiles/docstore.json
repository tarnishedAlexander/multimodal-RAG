{"docstore/data": {"1690b49b-e337-4fb2-81ba-6e9d22e0f0d2": {"__data__": {"id_": "1690b49b-e337-4fb2-81ba-6e9d22e0f0d2", "embedding": null, "metadata": {"page_label": "1", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "975f80d9-59dd-4926-a5da-0d4af8fa6e7a", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "c93b8a589edcd506e0260d4059616af4f5d614cbc95563a7ce7acf7d80880f97", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71330889-f270-4edf-84bf-46777db79be0", "node_type": "1", "metadata": {}, "hash": "82b69c3b22008428c044fc560ee14d26bce41c5a39e18d79feb0522aca039a65", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Kimera: an Open-Source Library for Real-Time\nMetric-Semantic Localization and Mapping\nAntoni Rosinol, Marcus Abate, Yun Chang, Luca Carlone\nFig. 1: Kimera is an open-source C++ library for real-time metric-semantic SLAM. It provides (a) visual-inertial state estimates at IMU\nrate, and a globally consistent and outlier-robust trajectory estimate, computes (b) a low-latency local mesh of the scene that can be used\nfor fast obstacle avoidance, and builds (c) a global semantically annotated 3D mesh, which accurately re\ufb02ects the ground truth model (d).\nAccepted for publication at ICRA 2020, please cite as follows:\nA. Rosinol, M. Abate, Y . Chang, L. Carlone\n\u201cKimera: an Open-Source Library for Real-Time Metric-Semantic Localization and Mapping\u201d,\nIEEE Int. Conf. Robot. Autom. (ICRA), 2020.\nAbstract\u2014 We provide an open-source C++ library for real-\ntime metric-semantic visual-inertial Simultaneous Localization\nAnd Mapping (SLAM). The library goes beyond existing visual\nand visual-inertial SLAM libraries ( e.g., ORB-SLAM, VINS-\nMono, OKVIS, ROVIO) by enabling mesh reconstruction and\nsemantic labeling in 3D. Kimera is designed with modularity\nin mind and has four key components: a visual-inertial odom-\netry (VIO) module for fast and accurate state estimation, a\nrobust pose graph optimizer for global trajectory estimation, a\nlightweight 3D mesher module for fast mesh reconstruction, and\na dense 3D metric-semantic reconstruction module. The mod-\nules can be run in isolation or in combination, hence Kimera\ncan easily fall back to a state-of-the-art VIO or a full SLAM\nsystem. Kimera runs in real-time on a CPU and produces a\n3D metric-semantic mesh from semantically labeled images,\nwhich can be obtained by modern deep learning methods.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1749, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "71330889-f270-4edf-84bf-46777db79be0": {"__data__": {"id_": "71330889-f270-4edf-84bf-46777db79be0", "embedding": null, "metadata": {"page_label": "1", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "975f80d9-59dd-4926-a5da-0d4af8fa6e7a", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "c93b8a589edcd506e0260d4059616af4f5d614cbc95563a7ce7acf7d80880f97", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1690b49b-e337-4fb2-81ba-6e9d22e0f0d2", "node_type": "1", "metadata": {"page_label": "1", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "324cab0abdc6e87fda21743f6af0c5a312647991563903049607e8c1626686fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd1de8c7-6e7d-45b0-94dc-926acd2a9931", "node_type": "1", "metadata": {}, "hash": "89f1a67af526baa0d7bcf1f9eae819c05b90e529158611a69a02e8a8df1b46eb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Kimera is designed with modularity\nin mind and has four key components: a visual-inertial odom-\netry (VIO) module for fast and accurate state estimation, a\nrobust pose graph optimizer for global trajectory estimation, a\nlightweight 3D mesher module for fast mesh reconstruction, and\na dense 3D metric-semantic reconstruction module. The mod-\nules can be run in isolation or in combination, hence Kimera\ncan easily fall back to a state-of-the-art VIO or a full SLAM\nsystem. Kimera runs in real-time on a CPU and produces a\n3D metric-semantic mesh from semantically labeled images,\nwhich can be obtained by modern deep learning methods. We\nhope that the \ufb02exibility, computational ef\ufb01ciency, robustness,\nand accuracy afforded by Kimera will build a solid basis for\nfuture metric-semantic SLAM and perception research, and\nwill allow researchers across multiple areas ( e.g., VIO, SLAM,\n3D reconstruction, segmentation) to benchmark and prototype\ntheir own efforts without having to start from scratch.\nSUPPLEMENTARY MATERIAL\nhttps://github.com/MIT-SPARK/Kimera\nhttps://www.youtube.com/watch?v=-5XxXRABXJs\nA. Rosinol, M. Abate, Y . Chang, L. Carlone are with the Laboratory for In-\nformation & Decision Systems (LIDS), Massachusetts Institute of Technol-\nogy, Cambridge, MA, USA, {arosinol,mabate,yunchang,lcarlone}@mit.edu\nThis work was partially funded by ARL DCIST CRA W911NF-17-2-\n0181, MIT Lincoln Laboratory, and \u2018la Caixa\u2019 Foundation (ID 100010434),\nunder the agreement LCF/BQ/AA18/11680088 (A. Rosinol).\nI. I NTRODUCTION\nMetric-semantic understanding is the capability to simul-\ntaneously estimate the 3D geometry of a scene and attach a\nsemantic label to objects and structures ( e.g., tables, walls).", "mimetype": "text/plain", "start_char_idx": 1115, "end_char_idx": 2821, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fd1de8c7-6e7d-45b0-94dc-926acd2a9931": {"__data__": {"id_": "fd1de8c7-6e7d-45b0-94dc-926acd2a9931", "embedding": null, "metadata": {"page_label": "1", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "975f80d9-59dd-4926-a5da-0d4af8fa6e7a", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "c93b8a589edcd506e0260d4059616af4f5d614cbc95563a7ce7acf7d80880f97", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71330889-f270-4edf-84bf-46777db79be0", "node_type": "1", "metadata": {"page_label": "1", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "7e978c0b8243a52b70d1005bfecafd1337b2c3214876e9a2f99ebd6822de7f32", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Chang, L. Carlone are with the Laboratory for In-\nformation & Decision Systems (LIDS), Massachusetts Institute of Technol-\nogy, Cambridge, MA, USA, {arosinol,mabate,yunchang,lcarlone}@mit.edu\nThis work was partially funded by ARL DCIST CRA W911NF-17-2-\n0181, MIT Lincoln Laboratory, and \u2018la Caixa\u2019 Foundation (ID 100010434),\nunder the agreement LCF/BQ/AA18/11680088 (A. Rosinol).\nI. I NTRODUCTION\nMetric-semantic understanding is the capability to simul-\ntaneously estimate the 3D geometry of a scene and attach a\nsemantic label to objects and structures ( e.g., tables, walls).\nGeometric information is critical for robots to navigate safely\nand to manipulate objects, while semantic information pro-\nvides the ideal level of abstraction for a robot to understand\nand execute human instructions ( e.g., \u201cbring me a cup of\ncoffee\u201d, \u201cexit from the red door\u201d) and to provide humans\nwith models of the environment that are easy to understand.\nDespite the unprecedented progress in geometric recon-\nstruction (e.g., SLAM [1], Structure from Motion [2], and\nMulti-View Stereo [3]) and deep-learning-based semantic\nsegmentation (e.g., [4]\u2013[10]), research in these two \ufb01elds has\ntraditionally proceeded in isolation. However, recently there\nhas been a growing interest towards research and applications\nat the intersection of these areas [1], [11]\u2013[15].\nThis growing interest motivated us to create and release\nKimera, a library for metric-semantic localization and map-\nping that combines the state of the art in geometric and\nsemantic understanding into a modern perception library.\nContrary to related efforts targeting visual-inertial odometry\n(VIO) and SLAM, we combine visual-inertial SLAM, mesh\nreconstruction, and semantic understanding. Our effort also\ncomplements approaches at the boundary between metric\nand semantic understanding in several aspects. First, while\nexisting efforts focus on RGB-D sensing, Kimera uses visual\narXiv:1910.02490v3  [cs.RO]  4 Mar 2020", "mimetype": "text/plain", "start_char_idx": 2243, "end_char_idx": 4211, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4347790c-3c18-49a5-bb21-dc3b6d1cd3e7": {"__data__": {"id_": "4347790c-3c18-49a5-bb21-dc3b6d1cd3e7", "embedding": null, "metadata": {"page_label": "2", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7d9b2c19-f852-466e-b181-07c2b7cb445a", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "2e2575f7c53414fbcc00d0624e08a5fafe8d45bf268bcd2bacebe1a3ffd940e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe5ab592-b3c9-48b4-826f-85b97b7dac10", "node_type": "1", "metadata": {}, "hash": "2c0d17e86bcc882b4c4dc7d661665c05e1d1d3b5ae2709efdc90c6c5a3091b03", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Method Sensors Back-end Geometry Sema-\nntics\nORB-SLAM [22] mono g2o points \u0017\nDSO [23] mono g2o points \u0017\nVINS-mono [24] mono/IMU Ceres points \u0017\nVINS-Fusion [25] mono/Stereo/IMU Ceres points \u0017\nROVIOLI [26] stereo/IMU EKF points \u0017\nElasticFusion [18] RGB-D alternation surfels \u0017\nV oxblox [27] RGB-D [26] TSDF \u0017\nSLAM++ [16] RGB-D alternation objects \u0013\nSemanticFusion [17] RGB-D [18] surfels \u0013\nMask-fusion [28] RGB-D [29] surfels \u0013\nSegMap [30] lidar GTSAM points/segments \u0013\nXIVO [31] mono/IMU EKF objects \u0013\nV oxblox++ [14] RGB-D [26] TSDF \u0013\nKimera mono/stereo/IMU GTSAM mesh/TSDF \u0013\nTABLE I: Related open-source libraries for visual and visual-\ninertial SLAM (top) and metric-semantic reconstruction (bottom).\n(RGB) and inertial sensing, which works well in a broader\nvariety of (indoor and outdoor) environments. Second, while\nrelated works [16]\u2013[18] require a GPU for 3D mapping, we\nprovide a fast, lightweight, and scalable CPU-based solution.\nFinally, we focus on robustness: we include state-of-the-\nart outlier rejection methods to ensure that Kimera executes\nrobustly and with minimal parameter tuning across a variety\nof scenarios, from real benchmarking datasets [19] to photo-\nrealistic simulations [20], [21].\nRelated Work. We refer the reader to Table I for a\nvisual comparison against existing VIO and visual-SLAM\nsystems, and to [1] for a broader review on SLAM. While\nearly work on metric-semantic understanding [11], [32] were\ndesigned for of\ufb02ine processing, recent years have seen a\nsurge of interest towards real-time metric-semantic mapping,\ntriggered by pioneering works such as SLAM++ [16].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1604, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fe5ab592-b3c9-48b4-826f-85b97b7dac10": {"__data__": {"id_": "fe5ab592-b3c9-48b4-826f-85b97b7dac10", "embedding": null, "metadata": {"page_label": "2", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7d9b2c19-f852-466e-b181-07c2b7cb445a", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "2e2575f7c53414fbcc00d0624e08a5fafe8d45bf268bcd2bacebe1a3ffd940e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4347790c-3c18-49a5-bb21-dc3b6d1cd3e7", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "4081be7d8a28990842068fe7f20ed54cd7f3055e756e30733f5420ad659c187c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "63d7c5e5-5ea1-4d52-b0fa-07f6a29d7ff9", "node_type": "1", "metadata": {}, "hash": "c8adeb8f64242a4883ec690f71549500a054dc155279476b6f65cdda20d4a7d7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Second, while\nrelated works [16]\u2013[18] require a GPU for 3D mapping, we\nprovide a fast, lightweight, and scalable CPU-based solution.\nFinally, we focus on robustness: we include state-of-the-\nart outlier rejection methods to ensure that Kimera executes\nrobustly and with minimal parameter tuning across a variety\nof scenarios, from real benchmarking datasets [19] to photo-\nrealistic simulations [20], [21].\nRelated Work. We refer the reader to Table I for a\nvisual comparison against existing VIO and visual-SLAM\nsystems, and to [1] for a broader review on SLAM. While\nearly work on metric-semantic understanding [11], [32] were\ndesigned for of\ufb02ine processing, recent years have seen a\nsurge of interest towards real-time metric-semantic mapping,\ntriggered by pioneering works such as SLAM++ [16]. Most\nof these works (i) rely on RGB-D cameras, (ii) use GPU\nprocessing, (iii) alternate tracking and mapping (\u201calterna-\ntion\u201d in Table I), and (iv) use voxel-based ( e.g., Trun-\ncated Signed Distance Function, TSDF), surfel, or ob-\nject representations. Examples include SemanticFusion [17],\nthe approach of Zheng et al. [15], Tateno et al. [33],\nand Li et al. [34], Fusion++ [35], Mask-fusion [28], Co-\nfusion [36], and MID-Fusion [37]. Recent work investigates\nCPU-based approaches, e.g., Wald et al. [38], PanopticFu-\nsion [39], and V oxblox++ [14]; these also rely on RGB-\nD sensing. A sparser set of contributions address other\nsensing modalities, including monocular cameras (e.g., CNN-\nSLAM [40], VSO [41], VITAMIN-E [42], XIVO [31]) and\nlidar (e.g., SemanticKitti [43], SegMap [30]). XIVO [31] and\nV oxblox++ [14] are the closest to our proposal. XIVO [31]\nis an EKF-based visual-inertial approach and produces an\nobject-based map.", "mimetype": "text/plain", "start_char_idx": 807, "end_char_idx": 2544, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "63d7c5e5-5ea1-4d52-b0fa-07f6a29d7ff9": {"__data__": {"id_": "63d7c5e5-5ea1-4d52-b0fa-07f6a29d7ff9", "embedding": null, "metadata": {"page_label": "2", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7d9b2c19-f852-466e-b181-07c2b7cb445a", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "2e2575f7c53414fbcc00d0624e08a5fafe8d45bf268bcd2bacebe1a3ffd940e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fe5ab592-b3c9-48b4-826f-85b97b7dac10", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "858b3e1b86a4dfd471e42391ace13c3a1e59c78ac181c4d0a744cb87d25a524b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "52a62b94-9292-4ded-9575-a06d26587a42", "node_type": "1", "metadata": {}, "hash": "1e703bc8f7c0b3b759c402629c8701742c907da9c71956a6a793aaef250ade63", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[33],\nand Li et al. [34], Fusion++ [35], Mask-fusion [28], Co-\nfusion [36], and MID-Fusion [37]. Recent work investigates\nCPU-based approaches, e.g., Wald et al. [38], PanopticFu-\nsion [39], and V oxblox++ [14]; these also rely on RGB-\nD sensing. A sparser set of contributions address other\nsensing modalities, including monocular cameras (e.g., CNN-\nSLAM [40], VSO [41], VITAMIN-E [42], XIVO [31]) and\nlidar (e.g., SemanticKitti [43], SegMap [30]). XIVO [31] and\nV oxblox++ [14] are the closest to our proposal. XIVO [31]\nis an EKF-based visual-inertial approach and produces an\nobject-based map. V oxblox++ [14] relies on RGB-D sensing,\nwheel odometry, and pre-built maps using maplab [26] to\nobtain visual-inertial pose estimates. Contrary to these works,\nKimera (i) provides a highly-accurate real-time optimization-\nbased VIO, (ii) uses a robust and versatile pose graph opti-\nmizer, and (iii) provides a lightweight mesh reconstruction.\nContribution. We release Kimera, an open-source C++\nlibrary that uses visual-inertial sensing to estimate the state\nof the robot and build a lightweight metric-semantic mesh\nmodel of the environment. The name Kimera stems from\nthe hybrid nature of our library, that uni\ufb01es state-of-the-\nart efforts across research areas, including VIO, pose graph\noptimization (PGO), mesh reconstruction, and 3D semantic\nsegmentation. Kimera includes four key modules:\n\u2022 Kimera-VIO: a VIO module for fast and accurate\nIMU-rate state estimation.", "mimetype": "text/plain", "start_char_idx": 1946, "end_char_idx": 3418, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "52a62b94-9292-4ded-9575-a06d26587a42": {"__data__": {"id_": "52a62b94-9292-4ded-9575-a06d26587a42", "embedding": null, "metadata": {"page_label": "2", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7d9b2c19-f852-466e-b181-07c2b7cb445a", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "2e2575f7c53414fbcc00d0624e08a5fafe8d45bf268bcd2bacebe1a3ffd940e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "63d7c5e5-5ea1-4d52-b0fa-07f6a29d7ff9", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "182fb71db5b86b0f5cb5cb1a87f8e9ba8d1b5d33862e7b4f5e969cf33269bf9f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "98430460-6eed-4a39-a767-90479f1408c3", "node_type": "1", "metadata": {}, "hash": "8c2ee126ff9dd7debf6fabbb96cd2aaa9d222f06fcb70de545d31ecb8a1a62a8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Contrary to these works,\nKimera (i) provides a highly-accurate real-time optimization-\nbased VIO, (ii) uses a robust and versatile pose graph opti-\nmizer, and (iii) provides a lightweight mesh reconstruction.\nContribution. We release Kimera, an open-source C++\nlibrary that uses visual-inertial sensing to estimate the state\nof the robot and build a lightweight metric-semantic mesh\nmodel of the environment. The name Kimera stems from\nthe hybrid nature of our library, that uni\ufb01es state-of-the-\nart efforts across research areas, including VIO, pose graph\noptimization (PGO), mesh reconstruction, and 3D semantic\nsegmentation. Kimera includes four key modules:\n\u2022 Kimera-VIO: a VIO module for fast and accurate\nIMU-rate state estimation. At its core, Kimera-VIO fea-\ntures a GTSAM-based VIO approach [44], using IMU-\npreintegration and structureless vision factors [45], and\nachieves top performance on the EuRoC dataset [19];\n\u2022 Kimera-RPGO: a robust pose graph optimization (RPGO)\nmethod that capitalizes on modern techniques for outlier\nrejection [46]. Kimera-RPGO adds a robustness layer that\navoids SLAM failures due to perceptual aliasing, and\nrelieves the user from time-consuming parameter tuning;\n\u2022 Kimera-Mesher: a module that computes a fast per-frame\nand multi-frame regularized 3D mesh to support obstacle\navoidance. The mesher builds on previous algorithms by\nthe authors and other groups [42], [47]\u2013[49];\n\u2022 Kimera-Semantics: a module that builds a slower-but-\nmore-accurate global 3D mesh using a volumetric ap-\nproach [27], and semantically annotates the 3D mesh using\n2D pixel-wise semantic segmentation.\nKimera can work both with of\ufb02ine datasets or online using\nthe Robot Operating System (ROS) [50]. It runs in real-time\non a CPU and provides useful debugging and visualization\ntools. Moreover, it is modular and allows replacing each\nmodule or executing them in isolation.", "mimetype": "text/plain", "start_char_idx": 2681, "end_char_idx": 4572, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "98430460-6eed-4a39-a767-90479f1408c3": {"__data__": {"id_": "98430460-6eed-4a39-a767-90479f1408c3", "embedding": null, "metadata": {"page_label": "2", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7d9b2c19-f852-466e-b181-07c2b7cb445a", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "2e2575f7c53414fbcc00d0624e08a5fafe8d45bf268bcd2bacebe1a3ffd940e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "52a62b94-9292-4ded-9575-a06d26587a42", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "504f2ba086e63eb40578d052b741da117fc07757f7b9d1ea698a9f0d035f4379", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f3b799e-b065-416f-818e-2e44ac00b6aa", "node_type": "1", "metadata": {}, "hash": "852057c34d536e341681de5af711e4d395b5dd183cf312cd0ba77e0b046d975c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The mesher builds on previous algorithms by\nthe authors and other groups [42], [47]\u2013[49];\n\u2022 Kimera-Semantics: a module that builds a slower-but-\nmore-accurate global 3D mesh using a volumetric ap-\nproach [27], and semantically annotates the 3D mesh using\n2D pixel-wise semantic segmentation.\nKimera can work both with of\ufb02ine datasets or online using\nthe Robot Operating System (ROS) [50]. It runs in real-time\non a CPU and provides useful debugging and visualization\ntools. Moreover, it is modular and allows replacing each\nmodule or executing them in isolation. For instance, it\ncan fall back to a VIO solution or can simply estimate a\ngeometric mesh if the semantic labels are not available.\nII. K IMERA\nFig. 2 shows Kimera\u2019s architecture. Kimera takes stereo\nframes and high-rate inertial measurements as input and\nreturns (i) a highly accurate state estimate at IMU rate, (ii)\na globally-consistent trajectory estimate, and (iii) multiple\nmeshes of the environment, including a fast local mesh and\na global semantically annotated mesh. Kimera is heavily\nparallelized and uses four threads to accommodate inputs\nand outputs at different rates ( e.g., IMU, frames, keyframes).\nHere we describe the architecture by threads , while the\ndescription of each module is given in the following sections.\nThe \ufb01rst thread includes the Kimera-VIO front-end (Sec-\ntion II-A) that takes stereo images and IMU data and\noutputs feature tracks and preintegrated IMU measurements.\nThe front-end also publishes IMU-rate state estimates. The\nsecond thread includes (i) the Kimera-VIO back-end, that\noutputs optimized state estimates, and (ii) Kimera-Mesher\n(Section II-C), that computes low-latency ( < 20ms) per-\nframe and multi-frame 3D meshes. These two threads allow\ncreating the per-frame mesh in Fig. 2(b) (which can also\ncome with semantic labels as in Fig. 2(c)), as well as\nthe multi-frame mesh in Fig. 2(d).", "mimetype": "text/plain", "start_char_idx": 4010, "end_char_idx": 5911, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0f3b799e-b065-416f-818e-2e44ac00b6aa": {"__data__": {"id_": "0f3b799e-b065-416f-818e-2e44ac00b6aa", "embedding": null, "metadata": {"page_label": "2", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7d9b2c19-f852-466e-b181-07c2b7cb445a", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "2e2575f7c53414fbcc00d0624e08a5fafe8d45bf268bcd2bacebe1a3ffd940e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "98430460-6eed-4a39-a767-90479f1408c3", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "188b60cd1b18ca7d7c881ad30f8b64b1f9818e7bad172b8f40eae0b31ea11a2b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Here we describe the architecture by threads , while the\ndescription of each module is given in the following sections.\nThe \ufb01rst thread includes the Kimera-VIO front-end (Sec-\ntion II-A) that takes stereo images and IMU data and\noutputs feature tracks and preintegrated IMU measurements.\nThe front-end also publishes IMU-rate state estimates. The\nsecond thread includes (i) the Kimera-VIO back-end, that\noutputs optimized state estimates, and (ii) Kimera-Mesher\n(Section II-C), that computes low-latency ( < 20ms) per-\nframe and multi-frame 3D meshes. These two threads allow\ncreating the per-frame mesh in Fig. 2(b) (which can also\ncome with semantic labels as in Fig. 2(c)), as well as\nthe multi-frame mesh in Fig. 2(d). The last two threads\noperate at slower rate and are designed to support low-\nfrequency functionalities, such as path planning. The third\nthread includes Kimera-RPGO (Section II-B), a robust PGO\nimplementation that detects loop closures, rejects outliers,\nand estimates a globally consistent trajectory (Fig. 2(a)). The\nlast thread includes Kimera-Semantics (Section II-D), that", "mimetype": "text/plain", "start_char_idx": 5189, "end_char_idx": 6289, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cff62621-5066-43e2-85ce-b121d0204585": {"__data__": {"id_": "cff62621-5066-43e2-85ce-b121d0204585", "embedding": null, "metadata": {"page_label": "3", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e78cd701-1c80-4882-82f0-232fb546ed58", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "9a8c103934ce3f26306787eceae4ec1bf40211164a27be9dba5f1c7e21ec9f28", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "903da76a-7bb8-423d-afbc-d54db070cf5d", "node_type": "1", "metadata": {}, "hash": "d68fd9e58957484586f5d7fea5478060a258815eb55892ade1c665148d41f1a4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fig. 2: Kimera\u2019s architecture. Kimera uses images and IMU data as input (shown on the left) and outputs (a) pose estimates and (b-e)\nmultiple metric-semantic reconstructions. Kimera has 4 key modules: Kimera-VIO, Kimera-RPGO, Kimera-Mesher, Kimera-Semantics.\nuses dense stereo and 2D semantic labels to obtain a re\ufb01ned\nmetric-semantic mesh, using Kimera-VIO\u2019s pose estimates.\nA. Kimera-VIO: Visual-Inertial Odometry Module\nKimera-VIO implements the keyframe-based maximum-\na-posteriori visual-inertial estimator presented in [45]. In\nour implementation, the estimator can perform both full\nsmoothing or \ufb01xed-lag smoothing, depending on the speci\ufb01ed\ntime horizon; we typically use the latter to bound the\nestimation time. We also extend [45] to work with both\nmonocular and stereo frames. Kimera-VIO includes a (visual\nand inertial) front-end which is in charge of processing the\nraw sensor data, and a back-end, that fuses the processed\nmeasurements to obtain an estimate of the state of the sensors\n(i.e., pose, velocity, and sensor biases).\n1) VIO Front-end: Our IMU front-end performs on-\nmanifold preintegration [45] to obtain compact preintegrated\nmeasurements of the relative state between two consecutive\nkeyframes from raw IMU data. The vision front-end detects\nShi-Tomasi corners [51], tracks them across frames using the\nLukas-Kanade tracker [52], \ufb01nds left-right stereo matches,\nand performs geometric veri\ufb01cation . We perform both\nmono(cular) veri\ufb01cation using 5-point RANSAC [53] and\nstereo veri\ufb01cation using 3-point RANSAC [54]; the code\nalso offers the option to use the IMU rotation and perform\nmono and stereo veri\ufb01cation using 2-point [55] and 1-point\nRANSAC, respectively. Feature detection, stereo matching,\nand geometric veri\ufb01cation are executed at each keyframe,\nwhile we only track features at intermediate frames.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1837, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "903da76a-7bb8-423d-afbc-d54db070cf5d": {"__data__": {"id_": "903da76a-7bb8-423d-afbc-d54db070cf5d", "embedding": null, "metadata": {"page_label": "3", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e78cd701-1c80-4882-82f0-232fb546ed58", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "9a8c103934ce3f26306787eceae4ec1bf40211164a27be9dba5f1c7e21ec9f28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cff62621-5066-43e2-85ce-b121d0204585", "node_type": "1", "metadata": {"page_label": "3", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "2f700818d10b9452b44e66690869efda70a0848f9caf74899ce6bb40cf7aedf4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c1eea650-23f6-45f2-a27c-68b98e637cb5", "node_type": "1", "metadata": {}, "hash": "709102a540a2943a73bd69b763e4dd04ffccbbe139a781e7fb5159131828342b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The vision front-end detects\nShi-Tomasi corners [51], tracks them across frames using the\nLukas-Kanade tracker [52], \ufb01nds left-right stereo matches,\nand performs geometric veri\ufb01cation . We perform both\nmono(cular) veri\ufb01cation using 5-point RANSAC [53] and\nstereo veri\ufb01cation using 3-point RANSAC [54]; the code\nalso offers the option to use the IMU rotation and perform\nmono and stereo veri\ufb01cation using 2-point [55] and 1-point\nRANSAC, respectively. Feature detection, stereo matching,\nand geometric veri\ufb01cation are executed at each keyframe,\nwhile we only track features at intermediate frames.\n2) VIO Back-end: At each keyframe, preintegrated IMU\nand visual measurements are added to a \ufb01xed-lag smoother\n(a factor graph) which constitutes our VIO back-end. We\nuse the preintegrated IMU model and the structureless vision\nmodel of [45]. The factor graph is solved using iSAM2 [56]\nin GTSAM [57]. At each iSAM2 iteration, the structureless\nvision model estimates the 3D position of the observed\nfeatures using DLT [58] and analytically eliminates the\ncorresponding 3D points from the VIO state [59]. Before\nelimination, degenerate points (i.e., points behind the camera\nor without enough parallax for triangulation) and outliers\n(i.e., points with large reprojection error) are removed, pro-\nviding an extra robustness layer. Finally, states that fall out of\nthe smoothing horizon are marginalized out using GTSAM.\nB. Kimera-RPGO: Robust Pose Graph Optimization Module\nKimera-RPGO is in charge of (i) detecting loop closures\nbetween the current and past keyframes, and (ii) computing\nglobally consistent keyframe poses using robust PGO.\n1) Loop Closure Detection: The loop closure detection\nrelies on the DBoW2 library [60] and uses a bag-of-word\nrepresentation to quickly detect putative loop closures.", "mimetype": "text/plain", "start_char_idx": 1241, "end_char_idx": 3045, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c1eea650-23f6-45f2-a27c-68b98e637cb5": {"__data__": {"id_": "c1eea650-23f6-45f2-a27c-68b98e637cb5", "embedding": null, "metadata": {"page_label": "3", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e78cd701-1c80-4882-82f0-232fb546ed58", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "9a8c103934ce3f26306787eceae4ec1bf40211164a27be9dba5f1c7e21ec9f28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "903da76a-7bb8-423d-afbc-d54db070cf5d", "node_type": "1", "metadata": {"page_label": "3", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "78559c5643ecd5f868001300b8c400224ce47a0b97e6072794f60f5e6f4a7eb0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Before\nelimination, degenerate points (i.e., points behind the camera\nor without enough parallax for triangulation) and outliers\n(i.e., points with large reprojection error) are removed, pro-\nviding an extra robustness layer. Finally, states that fall out of\nthe smoothing horizon are marginalized out using GTSAM.\nB. Kimera-RPGO: Robust Pose Graph Optimization Module\nKimera-RPGO is in charge of (i) detecting loop closures\nbetween the current and past keyframes, and (ii) computing\nglobally consistent keyframe poses using robust PGO.\n1) Loop Closure Detection: The loop closure detection\nrelies on the DBoW2 library [60] and uses a bag-of-word\nrepresentation to quickly detect putative loop closures. For\neach putative loop closure, we reject outlier loop closures\nusing mono and stereo geometric veri\ufb01cation (as described\nin Section II-A), and pass the remaining loop closures to\nthe robust PGO solver. Note that the resulting loop closures\ncan still contain outliers due to perceptual aliasing ( e.g., two\nidentical rooms on different \ufb02oors of a building).\n2) Robust PGO: This module is implemented in GTSAM,\nand includes a modern outlier rejection method, Incremental\nConsistent Measurement Set Maximization (PCM) [46], that\nwe tailor to a single-robot and online setup. We store sep-\narately the odometry edges (produced by Kimera-VIO) and\nthe loop closures (produced by the loop closure detection);\neach time the PGO is executed, we \ufb01rst select the largest set\nof consistent loop closures using a modi\ufb01ed version of PCM,\nand then execute GTSAM on the pose graph including the\nodometry and the consistent loop closures.\nPCM is designed for the multi-robot case and only checks", "mimetype": "text/plain", "start_char_idx": 2342, "end_char_idx": 4025, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2a76b8b0-3a7b-4ed0-94f2-4fd315547377": {"__data__": {"id_": "2a76b8b0-3a7b-4ed0-94f2-4fd315547377", "embedding": null, "metadata": {"page_label": "4", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d9e034f3-f435-456d-9c7d-9a03358f3a73", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "7bce8c9279103684b0e49f46543f3735b4598329aaf0ba8a381bb92bec32faba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "253e50c5-3d56-4612-96b0-88d48e5492b7", "node_type": "1", "metadata": {}, "hash": "7f0a6f1e5c522733af5751967a04c84a6406ee3daeffcf8dadd3301450babd76", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "that inter-robot loop closures are consistent. We developed\na C++ implementation of PCM that (i) adds an odometry\nconsistency check on the loop closures and (ii) incrementally\nupdates the set of consistent measurements to enable online\noperation. The odometry check veri\ufb01es that each loop closure\n(e.g., l1 in Fig. 2(a)) is consistent with the odometry (in red\nin the \ufb01gure): in the absence of noise, the poses along the\ncycle formed by the odometry and the loop l1 must compose\nto the identity. As in PCM, we \ufb02ag as outliers loops for\nwhich the error accumulated along the cycle is not consistent\nwith the measurement noise using a Chi-squared test. If\na loop detected at the current time t passes the odometry\ncheck, we test if it is pairwise consistent with previous loop\nclosures as in [46] ( e.g., check if loops l1 and l2 in Fig. 2(a)\nare consistent with each other). While PCM [46] builds an\nadjacency matrix A \u2208RL\u00d7L from scratch to keep track of\npairwise-consistent loops (where Lis the number of detected\nloop closures), we enable online operation by building the\nmatrix A incrementally. Each time a new loop is detected,\nwe add a row and column to the matrix A and only test\nthe new loop against the previous ones. Finally, we use the\nfast maximum clique implementation of [61] to compute the\nlargest set of consistent loop closures. The set of consistent\nmeasurements are added to the pose graph (together with the\nodometry) and optimized using Gauss-Newton.\nC. Kimera-Mesher: 3D Mesh Reconstruction\nKimera-Mesher can quickly generate two types of 3D\nmeshes: (i) a per-frame 3D mesh, and (ii) a multi-frame 3D\nmesh spanning the keyframes in the VIO \ufb01xed-lag smoother.\n1) Per-frame mesh: As in [47], we \ufb01rst perform a 2D\nDelaunay triangulation over the successfully tracked 2D\nfeatures (generated by the VIO front-end) in the current\nkeyframe.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1853, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "253e50c5-3d56-4612-96b0-88d48e5492b7": {"__data__": {"id_": "253e50c5-3d56-4612-96b0-88d48e5492b7", "embedding": null, "metadata": {"page_label": "4", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d9e034f3-f435-456d-9c7d-9a03358f3a73", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "7bce8c9279103684b0e49f46543f3735b4598329aaf0ba8a381bb92bec32faba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a76b8b0-3a7b-4ed0-94f2-4fd315547377", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "034b3db3f0ae3b6fb82e167d2c7b17535ce4b92649c6ff441bfdf8578701df1c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4faa2f83-321e-41ba-86f3-ea7411c93717", "node_type": "1", "metadata": {}, "hash": "f2eac8be17bf874f3a527fd87bacdbbe69ea6caa40e2cdbd33e5cda3ea231739", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Finally, we use the\nfast maximum clique implementation of [61] to compute the\nlargest set of consistent loop closures. The set of consistent\nmeasurements are added to the pose graph (together with the\nodometry) and optimized using Gauss-Newton.\nC. Kimera-Mesher: 3D Mesh Reconstruction\nKimera-Mesher can quickly generate two types of 3D\nmeshes: (i) a per-frame 3D mesh, and (ii) a multi-frame 3D\nmesh spanning the keyframes in the VIO \ufb01xed-lag smoother.\n1) Per-frame mesh: As in [47], we \ufb01rst perform a 2D\nDelaunay triangulation over the successfully tracked 2D\nfeatures (generated by the VIO front-end) in the current\nkeyframe. Then, we back-project the 2D Delaunay triangu-\nlation to generate a 3D mesh (Fig. 2(b)), using the 3D point\nestimates from the VIO back-end. While the per-frame mesh\nis designed to provide low-latency obstacle detection, we also\nprovide the option to semantically label the resulting mesh,\nby texturing the mesh with 2D labels (Fig. 2(c)).\n2) Multi-frame mesh: The multi-frame mesh fuses the\nper-frame meshes collected over the VIO receding horizon\ninto a single mesh (Fig. 2(d)). Both per-frame and multi-\nframe 3D meshes are encoded as a list of vertex positions,\ntogether with a list of triplets of vertex IDs to describe the\ntriangular faces. Assuming we already have a multi-frame\nmesh at time t\u22121, for each new per-frame 3D mesh that\nwe generate (at time t), we loop over its vertices and triplets\nand add vertices and triplets that are in the per-frame mesh\nbut are missing in the multi-frame one. Then we loop over\nthe multi-frame mesh vertices and update their 3D position\naccording to the latest VIO back-end estimates. Finally, we\nremove vertices and triplets corresponding to old features\nobserved outside the VIO time horizon.", "mimetype": "text/plain", "start_char_idx": 1225, "end_char_idx": 2993, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4faa2f83-321e-41ba-86f3-ea7411c93717": {"__data__": {"id_": "4faa2f83-321e-41ba-86f3-ea7411c93717", "embedding": null, "metadata": {"page_label": "4", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d9e034f3-f435-456d-9c7d-9a03358f3a73", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "7bce8c9279103684b0e49f46543f3735b4598329aaf0ba8a381bb92bec32faba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "253e50c5-3d56-4612-96b0-88d48e5492b7", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "98723581374a46ecbcfd26691dae7e2cbba2ea646a845d93c3da18a21d6c8c97", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a3a7c67-571b-4d7f-8f6c-3538fb7f7a53", "node_type": "1", "metadata": {}, "hash": "1ebc362e554c692eb4103a50fe63685129361b24c6c426feee600b5e86aca6fa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2) Multi-frame mesh: The multi-frame mesh fuses the\nper-frame meshes collected over the VIO receding horizon\ninto a single mesh (Fig. 2(d)). Both per-frame and multi-\nframe 3D meshes are encoded as a list of vertex positions,\ntogether with a list of triplets of vertex IDs to describe the\ntriangular faces. Assuming we already have a multi-frame\nmesh at time t\u22121, for each new per-frame 3D mesh that\nwe generate (at time t), we loop over its vertices and triplets\nand add vertices and triplets that are in the per-frame mesh\nbut are missing in the multi-frame one. Then we loop over\nthe multi-frame mesh vertices and update their 3D position\naccording to the latest VIO back-end estimates. Finally, we\nremove vertices and triplets corresponding to old features\nobserved outside the VIO time horizon. The result is an\nup-to-date 3D mesh spanning the keyframes in the current\nVIO time horizon. If planar surfaces are detected in the\nmesh, regularity factors [47] are added to the VIO back-\nend, which results in a tight coupling between VIO and mesh\nregularization, see [47] for further details.\nD. Kimera-Semantics: Metric-Semantic Segmentation\nWe adapt the bundled raycasting technique introduced in\n[27] to (i) build an accurate global 3D mesh (covering the\nentire trajectory), and (ii) semantically annotate the mesh.\n1) Global mesh: Our implementation builds on V oxblox\n[27] and uses a voxel-based (TSDF) model to \ufb01lter out\nnoise and extract the global mesh. At each keyframe, we\nuse dense stereo (semi-global matching [62]) to obtain a\n3D point cloud from the current stereo pair. Then we apply\nbundled raycasting using V oxblox [27], using the \u201cfast\u201d\noption discussed in [27]. This process is repeated at each\nkeyframe and produces a TSFD, from which a mesh is\nextracted using marching cubes [63].", "mimetype": "text/plain", "start_char_idx": 2194, "end_char_idx": 3997, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2a3a7c67-571b-4d7f-8f6c-3538fb7f7a53": {"__data__": {"id_": "2a3a7c67-571b-4d7f-8f6c-3538fb7f7a53", "embedding": null, "metadata": {"page_label": "4", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d9e034f3-f435-456d-9c7d-9a03358f3a73", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "7bce8c9279103684b0e49f46543f3735b4598329aaf0ba8a381bb92bec32faba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4faa2f83-321e-41ba-86f3-ea7411c93717", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "b88cc3f5c15f3686e4071dee40f605f3b23f13b11c73f95278544e0cd04348a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "92988799-7642-4005-9b93-d072a5b82633", "node_type": "1", "metadata": {}, "hash": "80e9e1925c45e8492c3a518930b3c484c730f75607b16a5d60836c631e969f85", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "D. Kimera-Semantics: Metric-Semantic Segmentation\nWe adapt the bundled raycasting technique introduced in\n[27] to (i) build an accurate global 3D mesh (covering the\nentire trajectory), and (ii) semantically annotate the mesh.\n1) Global mesh: Our implementation builds on V oxblox\n[27] and uses a voxel-based (TSDF) model to \ufb01lter out\nnoise and extract the global mesh. At each keyframe, we\nuse dense stereo (semi-global matching [62]) to obtain a\n3D point cloud from the current stereo pair. Then we apply\nbundled raycasting using V oxblox [27], using the \u201cfast\u201d\noption discussed in [27]. This process is repeated at each\nkeyframe and produces a TSFD, from which a mesh is\nextracted using marching cubes [63].\n2) Semantic annotation: Kimera-Semantics uses 2D se-\nmantically labeled images (produced at each keyframe) to\nsemantically annotate the global mesh; the 2D semantic\nlabels can be obtained using off-the-shelf tools for pixel-level\n2D semantic segmentation, e.g., deep neural networks [7]\u2013\n[9], [64]\u2013[69] or classical MRF-based approaches [70]. To\nthis end, during the bundled raycasting, we also propagate\nthe semantic labels. Using the 2D semantic segmentation, we\nattach a label to each 3D point produced by the dense stereo.\nThen, for each bundle of rays in the bundled raycasting, we\nbuild a vector of label probabilities from the frequency of\nthe observed labels in the bundle. We then propagate this\ninformation along the ray only within the TSDF truncation\ndistance ( i.e., near the surface) to spare computation. In\nother words, we spare the computational effort of updating\nprobabilities for the \u201cempty\u201d label. While traversing the\nvoxels along the ray, we use a Bayesian update to update\nthe label probabilities at each voxel, similar to [17]. After\nbundled semantic raycasting, each voxel has a vector of label\nprobabilities, from which we extract the most likely label.", "mimetype": "text/plain", "start_char_idx": 3288, "end_char_idx": 5178, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "92988799-7642-4005-9b93-d072a5b82633": {"__data__": {"id_": "92988799-7642-4005-9b93-d072a5b82633", "embedding": null, "metadata": {"page_label": "4", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d9e034f3-f435-456d-9c7d-9a03358f3a73", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "7bce8c9279103684b0e49f46543f3735b4598329aaf0ba8a381bb92bec32faba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a3a7c67-571b-4d7f-8f6c-3538fb7f7a53", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "d26800b103430044266716e489db2b83e71a58f826caa7a7c48184c983ef5345", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To\nthis end, during the bundled raycasting, we also propagate\nthe semantic labels. Using the 2D semantic segmentation, we\nattach a label to each 3D point produced by the dense stereo.\nThen, for each bundle of rays in the bundled raycasting, we\nbuild a vector of label probabilities from the frequency of\nthe observed labels in the bundle. We then propagate this\ninformation along the ray only within the TSDF truncation\ndistance ( i.e., near the surface) to spare computation. In\nother words, we spare the computational effort of updating\nprobabilities for the \u201cempty\u201d label. While traversing the\nvoxels along the ray, we use a Bayesian update to update\nthe label probabilities at each voxel, similar to [17]. After\nbundled semantic raycasting, each voxel has a vector of label\nprobabilities, from which we extract the most likely label.\nThe metric-semantic mesh is \ufb01nally extracted using marching\ncubes [63]. The resulting mesh is signi\ufb01cantly more accurate\nthan the multi-frame mesh of Section II-C, but it is slower\nto compute ( \u22480.1s, see Section III-D).\nE. Debugging Tools\nWhile we limit the discussion for space reasons, it is worth\nmentioning that Kimera also provides an open-source suite\nof evaluation tools for debugging, visualization, and bench-\nmarking of VIO, SLAM, and metric-semantic reconstruction.\nKimera includes a Continuous Integration server (Jenkins)\nthat asserts the quality of the code (compilation, unit tests),\nbut also automatically evaluates Kimera-VIO and Kimera-\nRPGO on the EuRoC\u2019s datasets using evo [71]. Moreover,\nwe provide Jupyter Notebooks to visualize intermediate VIO\nstatistics (e.g., quality of the feature tracks, IMU preintegra-\ntion errors), as well as to automatically assess the quality of\nthe 3D reconstruction using Open3D [72].\nIII. E XPERIMENTAL EVALUATION\nSection III-A shows that (i) Kimera attains state-of-the-\nart state estimation performance and (ii) our robust PGO", "mimetype": "text/plain", "start_char_idx": 4341, "end_char_idx": 6263, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0240a1ae-7fd4-49aa-b10d-8edf42ba1e50": {"__data__": {"id_": "0240a1ae-7fd4-49aa-b10d-8edf42ba1e50", "embedding": null, "metadata": {"page_label": "5", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c966173-c71d-4a61-8c00-afcb6570a42a", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "3766c2aa6648ba54bf9e0427f7b82de6a617b15edeb53293652418411f445e3c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ef8fda7d-8ef3-4cfc-bbe3-187c4c8d8c8e", "node_type": "1", "metadata": {}, "hash": "47072ed18b0c97e929796a7c1acad70daf77fdc9cf3df685daaeeecec6b6e3a1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "TABLE II: RMSE of state-of-the-art VIO pipelines (reported from\n[77] and [24]) compared to Kimera, on the EuRoC dataset. In\nbold the best result for each category: \ufb01xed-lag smoothing, full\nsmoothing, and PGO with loop closure. \u00d7 indicates failure.\nRMSE ATE [m]\nFixed-lag Smoothing Full Smoothing Loop Closure\nSeq.\nOKVIS\nMSCKF\nROVIO\nVINS-\nMono\nKimera-\nVIO\nSVO-\nGTSAM\nKimera-\nVIO\nVINS-\nLC\nKimera-\nRPGO\nMH 1 0.16 0.42 0.21 0.15 0.11 0.05 0.04 0.12 0.08\nMH 2 0.22 0.45 0.25 0.15 0.10 0.03 0.07 0.12 0.09\nMH 3 0.24 0.23 0.25 0.22 0.16 0.12 0.12 0.13 0.11\nMH 4 0.34 0.37 0.49 0.32 0.24 0.13 0.27 0.18 0.15\nMH 5 0.47 0.48 0.52 0.30 0.35 0.16 0.20 0.21 0.24\nV1 1 0.09 0.34 0.10 0.08 0.05 0.07 0.06 0.06 0.05\nV1 2 0.20 0.20 0.10 0.11 0.08 0.11 0.07 0.08 0.11\nV1 3 0.24 0.67 0.14 0.18 0.07 \u00d7 0.09 0.19 0.12\nV2 1 0.13 0.10 0.12 0.08 0.08 0.07 0.07 0.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 839, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ef8fda7d-8ef3-4cfc-bbe3-187c4c8d8c8e": {"__data__": {"id_": "ef8fda7d-8ef3-4cfc-bbe3-187c4c8d8c8e", "embedding": null, "metadata": {"page_label": "5", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c966173-c71d-4a61-8c00-afcb6570a42a", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "3766c2aa6648ba54bf9e0427f7b82de6a617b15edeb53293652418411f445e3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0240a1ae-7fd4-49aa-b10d-8edf42ba1e50", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "4e0df50659695e8930644ddde6e1d117d2c4d82388c0ca1527b8836ce79f4796", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0bc84487-bccd-4395-b278-b1b2f292779c", "node_type": "1", "metadata": {}, "hash": "a47bb50607892ca7923a974a06dff998396a31294382b736ddb1af90699869d0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "15\nMH 5 0.47 0.48 0.52 0.30 0.35 0.16 0.20 0.21 0.24\nV1 1 0.09 0.34 0.10 0.08 0.05 0.07 0.06 0.06 0.05\nV1 2 0.20 0.20 0.10 0.11 0.08 0.11 0.07 0.08 0.11\nV1 3 0.24 0.67 0.14 0.18 0.07 \u00d7 0.09 0.19 0.12\nV2 1 0.13 0.10 0.12 0.08 0.08 0.07 0.07 0.08 0.07\nV2 2 0.16 0.16 0.14 0.16 0.10 \u00d7 0.09 0.16 0.10\nV2 3 0.29 1.13 0.14 0.27 0.21 \u00d7 0.19 1.39 0.19\nrelieves the user from time-consuming parameter tuning.\nSection III-B demonstrates Kimera\u2019s 3D mesh reconstruction\non EuRoC, using the subset of scenes providing a ground-\ntruth point cloud. Section III-C inspects Kimera\u2019s 3D metric-\nsemantic reconstruction using a photo-realistic simulator\n(see video attachment), which provides ground-truth 3D\nsemantics. Finally, Section III-D highlights Kimera\u2019s real-\ntime performance and analyzes the runtime of each module.\nA. Pose Estimation Performance\nTable II compares the Root Mean Squared Error (RMSE)\nof the Absolute Translation Error (ATE) of Kimera-\nVIO against state-of-the-art open-source VIO pipelines:\nOKVIS [73], MSCKF [74], ROVIO [75], VINS-Mono [24],\nand SVO-GTSAM [76] using the independently reported\nvalues in [77] and the self-reported values in [24].", "mimetype": "text/plain", "start_char_idx": 597, "end_char_idx": 1753, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0bc84487-bccd-4395-b278-b1b2f292779c": {"__data__": {"id_": "0bc84487-bccd-4395-b278-b1b2f292779c", "embedding": null, "metadata": {"page_label": "5", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c966173-c71d-4a61-8c00-afcb6570a42a", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "3766c2aa6648ba54bf9e0427f7b82de6a617b15edeb53293652418411f445e3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ef8fda7d-8ef3-4cfc-bbe3-187c4c8d8c8e", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "4b263b8f4a7d2778ade0193bff675bb6c3888666d40fbfaf3282ecc4f9d348e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5bc6cf5f-3375-4439-b922-13ad090950ca", "node_type": "1", "metadata": {}, "hash": "95dd85b010abfa81b7b97c28768c11ae8dd9cdf8b69093fffd86a65655145ad3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Section III-B demonstrates Kimera\u2019s 3D mesh reconstruction\non EuRoC, using the subset of scenes providing a ground-\ntruth point cloud. Section III-C inspects Kimera\u2019s 3D metric-\nsemantic reconstruction using a photo-realistic simulator\n(see video attachment), which provides ground-truth 3D\nsemantics. Finally, Section III-D highlights Kimera\u2019s real-\ntime performance and analyzes the runtime of each module.\nA. Pose Estimation Performance\nTable II compares the Root Mean Squared Error (RMSE)\nof the Absolute Translation Error (ATE) of Kimera-\nVIO against state-of-the-art open-source VIO pipelines:\nOKVIS [73], MSCKF [74], ROVIO [75], VINS-Mono [24],\nand SVO-GTSAM [76] using the independently reported\nvalues in [77] and the self-reported values in [24]. Note\nthat these algorithms use a monocular camera, while we use\na stereo camera. We align the estimated and ground-truth\ntrajectories using an SE(3) transformation before evaluating\nthe errors. Using a Sim(3) alignment, as in [77], would result\nin an even smaller error for Kimera: we preferred the SE(3)\nalignment, since it is more appropriate for VIO, where the\nscale is observable thanks to the IMU. We group the tech-\nniques depending on whether they use \ufb01xed-lag smoothing,\nfull smoothing, and loop closures. Kimera-VIO and Kimera-\nRPGO achieve top performance across the spectrum.\nFurthermore, Kimera-RPGO ensures robust performance,\nand is less sensitive to loop closure parameter tuning. Ta-\nble III shows the PGO accuracy with and without outlier\nrejection (PCM) for different values of the loop closure\nthreshold \u03b1used in DBoW2. Small values of \u03b1lead to more\nloop closure detections, but these are less conservative (more\noutliers). Table III shows that, by using PCM, Kimera-RPGO\nis fairly insensitive to the choice of \u03b1. The results in Table II\nuse \u03b1= 0.001.", "mimetype": "text/plain", "start_char_idx": 997, "end_char_idx": 2824, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5bc6cf5f-3375-4439-b922-13ad090950ca": {"__data__": {"id_": "5bc6cf5f-3375-4439-b922-13ad090950ca", "embedding": null, "metadata": {"page_label": "5", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c966173-c71d-4a61-8c00-afcb6570a42a", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "3766c2aa6648ba54bf9e0427f7b82de6a617b15edeb53293652418411f445e3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0bc84487-bccd-4395-b278-b1b2f292779c", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "ea415eec3c3928e3da48ef9aa4639e6ee7024e70e12ca00db82dbefc75d275e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "01f28555-0d02-4c95-a49c-5e18c154032d", "node_type": "1", "metadata": {}, "hash": "85aecda41660df495834499aca8b748ae9ac24a5eeede54ba7bd80525d31066f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We group the tech-\nniques depending on whether they use \ufb01xed-lag smoothing,\nfull smoothing, and loop closures. Kimera-VIO and Kimera-\nRPGO achieve top performance across the spectrum.\nFurthermore, Kimera-RPGO ensures robust performance,\nand is less sensitive to loop closure parameter tuning. Ta-\nble III shows the PGO accuracy with and without outlier\nrejection (PCM) for different values of the loop closure\nthreshold \u03b1used in DBoW2. Small values of \u03b1lead to more\nloop closure detections, but these are less conservative (more\noutliers). Table III shows that, by using PCM, Kimera-RPGO\nis fairly insensitive to the choice of \u03b1. The results in Table II\nuse \u03b1= 0.001.\nB. Geometric Reconstruction\nWe use the ground truth point cloud available in the\nEuRoC V1 and V2 datasets to assess the quality of the\nTABLE III: RMSE ATE [m] vs. loop closure threshold \u03b1(V1 01).\n\u03b1=10 \u03b1=1 \u03b1=0.1 \u03b1=0.01 \u03b1=0.001\nPGO w/o PCM 0.05 0.45 1.74 1.59 1.59\nKimera-RPGO 0.05 0.05 0.05 0.045 0.049\n3D meshes produced by Kimera. We evaluate each mesh\nagainst the ground truth using the accuracy and completeness\nmetrics as in [78, Sec. 4.3]: (i) we compute a point cloud by\nsampling our mesh with a uniform density of 103 points/m2,\n(ii) we register the estimated and the ground truth clouds with\nICP [79] using CloudCompare [80], and (iii) we evaluate the\naverage distance from ground truth point cloud to its nearest\nneighbor in the estimated point cloud (accuracy), and vice-\nversa (completeness). Fig. 3(a) shows the estimated cloud\n(corresponding to the global mesh of Kimera-Semantics on\nV1 01) color-coded by the distance to the closest point in the\nground-truth cloud (accuracy); Fig.", "mimetype": "text/plain", "start_char_idx": 2157, "end_char_idx": 3820, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "01f28555-0d02-4c95-a49c-5e18c154032d": {"__data__": {"id_": "01f28555-0d02-4c95-a49c-5e18c154032d", "embedding": null, "metadata": {"page_label": "5", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c966173-c71d-4a61-8c00-afcb6570a42a", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "3766c2aa6648ba54bf9e0427f7b82de6a617b15edeb53293652418411f445e3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5bc6cf5f-3375-4439-b922-13ad090950ca", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "814d5494f1b4fa76c6103c2f8fe8d8e689c46c1ba48f65961374c73bfd1ddb82", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "602e189f-81c5-4eb5-917b-cbcb0fbd7943", "node_type": "1", "metadata": {}, "hash": "ae62fcc8b740f150be38fd992c9ec57ee48549533aa3699050a625612d41eb48", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We evaluate each mesh\nagainst the ground truth using the accuracy and completeness\nmetrics as in [78, Sec. 4.3]: (i) we compute a point cloud by\nsampling our mesh with a uniform density of 103 points/m2,\n(ii) we register the estimated and the ground truth clouds with\nICP [79] using CloudCompare [80], and (iii) we evaluate the\naverage distance from ground truth point cloud to its nearest\nneighbor in the estimated point cloud (accuracy), and vice-\nversa (completeness). Fig. 3(a) shows the estimated cloud\n(corresponding to the global mesh of Kimera-Semantics on\nV1 01) color-coded by the distance to the closest point in the\nground-truth cloud (accuracy); Fig. 3(b) shows the ground-\ntruth cloud, color-coded with the distance to the closest-point\nin the estimated cloud (completeness).\nFig. 3: (a) Kimera\u2019s 3D mesh color-coded by the distance to the\nground-truth point cloud. (b) Ground-truth point cloud color-coded\nby the distance to the estimated cloud. EuRoC V1 01 dataset.\nTable IV provides a quantitative comparison between the\nfast multi-frame mesh produced by Kimera-Mesher and the\nslow mesh produced via TSDF by Kimera-Semantics. To\nobtain a complete mesh from Kimera-Mesher we set a large\nVIO horizon ( i.e., we perform full smoothing). As expected\nfrom Fig. 3(a), the global mesh from Kimera-Semantics is\nvery accurate, with an average error of 0.35 \u22120.48m across\ndatasets. Kimera-Mesher produces a more noisy mesh (up\nto 24% error increase), but requires two orders of magnitude\nless time to compute (see Section III-D).\nTABLE IV: Evaluation of Kimera multi-frame and global meshes\u2019\ncompleteness [78, Sec. 4.3.3] with an ICP threshold of 1.0m.\nRMSE [m] Relative\nImprovement [%]Seq.", "mimetype": "text/plain", "start_char_idx": 3157, "end_char_idx": 4854, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "602e189f-81c5-4eb5-917b-cbcb0fbd7943": {"__data__": {"id_": "602e189f-81c5-4eb5-917b-cbcb0fbd7943", "embedding": null, "metadata": {"page_label": "5", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c966173-c71d-4a61-8c00-afcb6570a42a", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "3766c2aa6648ba54bf9e0427f7b82de6a617b15edeb53293652418411f445e3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "01f28555-0d02-4c95-a49c-5e18c154032d", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "27a42a7f0beb65ed86cff27ba7a567a07c2d2355ac75271281beebf2638d82c3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To\nobtain a complete mesh from Kimera-Mesher we set a large\nVIO horizon ( i.e., we perform full smoothing). As expected\nfrom Fig. 3(a), the global mesh from Kimera-Semantics is\nvery accurate, with an average error of 0.35 \u22120.48m across\ndatasets. Kimera-Mesher produces a more noisy mesh (up\nto 24% error increase), but requires two orders of magnitude\nless time to compute (see Section III-D).\nTABLE IV: Evaluation of Kimera multi-frame and global meshes\u2019\ncompleteness [78, Sec. 4.3.3] with an ICP threshold of 1.0m.\nRMSE [m] Relative\nImprovement [%]Seq. Multi-Frame Global\nV1 01 0.482 0.364 24.00\nV1 02 0.374 0.384 -2.00\nV1 03 0.451 0.353 21.00\nV2 01 0.465 0.480 -3.00\nV2 02 0.491 0.432 12.00\nV2 03 0.530 0.411 22.00", "mimetype": "text/plain", "start_char_idx": 4300, "end_char_idx": 5017, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4111a776-4110-4df7-a783-5edfee6938d5": {"__data__": {"id_": "4111a776-4110-4df7-a783-5edfee6938d5", "embedding": null, "metadata": {"page_label": "6", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "341db2a2-ea1d-4906-bdd8-c9a83dceda29", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "15570c1d736fea8599e6eeb955f29c34d33961020be2279a4cc8e19217f8a53c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0fc82595-3163-4a2a-8679-414a59cdea05", "node_type": "1", "metadata": {}, "hash": "6b3c34882d4941145cf1e4187f22136571a0a281f029fa28d3c34e2966e389aa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "C. Semantic Reconstruction\nTo evaluate the accuracy of the metric-semantic recon-\nstruction from Kimera-Semantics, we use a photo-realistic\nUnity-based simulator provided by MIT Lincoln Lab, that\nprovides sensor streams (in ROS) and ground truth for both\nthe geometry and the semantics of the scene, and has an\ninterface similar to [20], [21]. To avoid biasing the results\ntowards a particular 2D semantic segmentation method, we\nuse ground truth 2D semantic segmentations and we refer\nthe reader to [70] for potential alternatives.\nKimera-Semantics builds a 3D mesh from the VIO pose\nestimates, and uses a combination of dense stereo and bun-\ndled raycasting. We evaluate the impact of each of these com-\nponents by running three different experiments. First, we use\nKimera-Semantics with ground-truth (GT) poses and ground-\ntruth depth maps (available in simulation) to assess the initial\nloss of performance due to bundled raycasting. Second,\nwe use Kimera-VIO\u2019s pose estimates. Finally, we use the\nfull Kimera-Semantics pipeline including dense stereo. To\nanalyze the semantic performance, we calculate the mean\nIntersection over Union (mIoU) [13], and the overall portion\nof correctly labeled points (Acc) [81]. We also report the\nATE to correlate the results with the drift incurred by\nKimera-VIO. Finally, we evaluate the metric reconstruction\nregistering the estimated mesh with the ground truth and\ncomputing the RMSE for the points as in Section III-B.\nTable V summarizes our \ufb01ndings and shows that bundled\nraycasting results in a small drop in performance both\ngeometrically (<8cm error on the 3D mesh) as well as se-\nmantically (accuracy >94%). Using Kimera-VIO also results\nin negligible loss in performance since our VIO has a small\ndrift ( < 0.2%, 4cm for a 32m long trajectory). Certainly,\nthe biggest drop in performance is due to the use of dense\nstereo. Dense stereo [62] has dif\ufb01culties resolving the depth\nof texture-less regions such as walls, which are frequent in\nsimulated scenes.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2005, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0fc82595-3163-4a2a-8679-414a59cdea05": {"__data__": {"id_": "0fc82595-3163-4a2a-8679-414a59cdea05", "embedding": null, "metadata": {"page_label": "6", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "341db2a2-ea1d-4906-bdd8-c9a83dceda29", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "15570c1d736fea8599e6eeb955f29c34d33961020be2279a4cc8e19217f8a53c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4111a776-4110-4df7-a783-5edfee6938d5", "node_type": "1", "metadata": {"page_label": "6", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "dce9839851a1d0c5923b975586dee4ca644501f529a067d3d0089c7c558c6fb1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51ce2bd6-2b47-4c46-b7ee-83a8b0ef44b9", "node_type": "1", "metadata": {}, "hash": "fcbd7a60816d2ecd2b8162b69359fb7a5263de651b69e63a7be4f85542d1aa26", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Finally, we evaluate the metric reconstruction\nregistering the estimated mesh with the ground truth and\ncomputing the RMSE for the points as in Section III-B.\nTable V summarizes our \ufb01ndings and shows that bundled\nraycasting results in a small drop in performance both\ngeometrically (<8cm error on the 3D mesh) as well as se-\nmantically (accuracy >94%). Using Kimera-VIO also results\nin negligible loss in performance since our VIO has a small\ndrift ( < 0.2%, 4cm for a 32m long trajectory). Certainly,\nthe biggest drop in performance is due to the use of dense\nstereo. Dense stereo [62] has dif\ufb01culties resolving the depth\nof texture-less regions such as walls, which are frequent in\nsimulated scenes. Fig. 4 shows the confusion matrix when\nrunning Kimera-Semantics with Kimera-VIO and ground-\ntruth depth (Fig. 4(a)), compared with using dense stereo\n(Fig. 4(b)). Large values in the confusion matrix appear\nbetween Wall/Shelf and Floor/Wall. This is exactly where\ndense stereo suffers the most; texture-less walls are dif\ufb01cult\nto reconstruct and are close to shelves and \ufb02oor, resulting in\nincreased geometric and semantic errors.\nFig. 4: Confusion matrices for Kimera-Semantics using bundled\nraycasting and (a) ground truth stereo depth or (b) dense stereo [62].\nBoth experiments use ground-truth 2D semantics. Values are satu-\nrated to 104 for visualization purposes.\nTABLE V: Evaluation of Kimera-Semantics.\nKimera-Semantics using:\nMetrics\nGT Depth\nGT Poses\nGT Depth\nKimera-VIO\nDense-Stereo\nKimera-VIO\nSemantic mIoU [%] 80.10 80.03 57.23\nAcc [%] 94.68 94.50 80.74\nGeometric ATE [m] 0.0 0.04 0.04\nRMSE [m] 0.079 0.131 0.215\nFig. 5: Runtime breakdown for Kimera-VIO, RPGO, and Mesher.\nD. Timing\nFig.", "mimetype": "text/plain", "start_char_idx": 1304, "end_char_idx": 3006, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "51ce2bd6-2b47-4c46-b7ee-83a8b0ef44b9": {"__data__": {"id_": "51ce2bd6-2b47-4c46-b7ee-83a8b0ef44b9", "embedding": null, "metadata": {"page_label": "6", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "341db2a2-ea1d-4906-bdd8-c9a83dceda29", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "15570c1d736fea8599e6eeb955f29c34d33961020be2279a4cc8e19217f8a53c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0fc82595-3163-4a2a-8679-414a59cdea05", "node_type": "1", "metadata": {"page_label": "6", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "ba39166aa35ce5ac4694922c5cf7768b7528124aaf1b3ab17512b2997eba9332", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83ac7da0-6cc6-4c6d-a633-d805e2d24be0", "node_type": "1", "metadata": {}, "hash": "8216b7b8f36f87151d894a668398b144490dd02b2a477afa03df3837fa561c12", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Both experiments use ground-truth 2D semantics. Values are satu-\nrated to 104 for visualization purposes.\nTABLE V: Evaluation of Kimera-Semantics.\nKimera-Semantics using:\nMetrics\nGT Depth\nGT Poses\nGT Depth\nKimera-VIO\nDense-Stereo\nKimera-VIO\nSemantic mIoU [%] 80.10 80.03 57.23\nAcc [%] 94.68 94.50 80.74\nGeometric ATE [m] 0.0 0.04 0.04\nRMSE [m] 0.079 0.131 0.215\nFig. 5: Runtime breakdown for Kimera-VIO, RPGO, and Mesher.\nD. Timing\nFig. 5 reports the timing performance of Kimera\u2019s mod-\nules. The IMU front-end requires around 40\u00b5s for prein-\ntegration, hence can generate state estimates at IMU rate\n(> 200Hz ). The vision front-end module shows a bi-\nmodal distribution since, for every frame, we just perform\nfeature tracking (which takes an average of 4.5ms), while, at\nkeyframe rate, we perform feature detection, stereo matching,\nand geometric veri\ufb01cation, which, combined, take an average\nof 45ms. Kimera-Mesher is capable of generating per-frame\n3D meshes in less than 5ms, while building the multi-frame\nmesh takes 15ms on average. The back-end solves the factor-\ngraph optimization in less than 40ms. Kimera-RPGO and\nKimera-Semantics run on slower threads since their outputs\nare not required for time-critical actions ( e.g., control, ob-\nstacle avoidance). Kimera-RPGO took an average of 55ms\nin our experiments on EuRoC, but in general its runtime\ndepends on the size of the pose graph. Finally, Kimera-\nSemantics (not reported in \ufb01gure for clarity) takes an average\nof 0.1s to update the global metric-semantic mesh at each\nkeyframe, fusing a 720 \u00d7480 dense depth image, as the one\nproduced by our simulator.\nIV.", "mimetype": "text/plain", "start_char_idx": 2570, "end_char_idx": 4196, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "83ac7da0-6cc6-4c6d-a633-d805e2d24be0": {"__data__": {"id_": "83ac7da0-6cc6-4c6d-a633-d805e2d24be0", "embedding": null, "metadata": {"page_label": "6", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "341db2a2-ea1d-4906-bdd8-c9a83dceda29", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "15570c1d736fea8599e6eeb955f29c34d33961020be2279a4cc8e19217f8a53c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "51ce2bd6-2b47-4c46-b7ee-83a8b0ef44b9", "node_type": "1", "metadata": {"page_label": "6", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "488daa14d07a3213f2332b9d2e2126d500033050a6675df9dadfe2816c62d7af", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Kimera-Mesher is capable of generating per-frame\n3D meshes in less than 5ms, while building the multi-frame\nmesh takes 15ms on average. The back-end solves the factor-\ngraph optimization in less than 40ms. Kimera-RPGO and\nKimera-Semantics run on slower threads since their outputs\nare not required for time-critical actions ( e.g., control, ob-\nstacle avoidance). Kimera-RPGO took an average of 55ms\nin our experiments on EuRoC, but in general its runtime\ndepends on the size of the pose graph. Finally, Kimera-\nSemantics (not reported in \ufb01gure for clarity) takes an average\nof 0.1s to update the global metric-semantic mesh at each\nkeyframe, fusing a 720 \u00d7480 dense depth image, as the one\nproduced by our simulator.\nIV. C ONCLUSION\nKimera is an open-source C++ library for metric-semantic\nSLAM. It includes state-of-the-art implementations of visual-\ninertial odometry, robust pose graph optimization, mesh\nreconstruction, and 3D semantic labeling. It runs in real-time\non a CPU and provides a suite of continuous integration and\nbenchmarking tools. We hope Kimera can provide a solid\nbasis for future research on robot perception, and an easy-\nto-use infrastructure for researchers across communities.\nAcknowledgments. We are thankful to Dan Grif\ufb01th, Ben\nSmith, Arjun Majumdar, and Zac Ravichandran for kindly\nsharing the photo-realistic simulator, and to Winter Guerra\nand Varun Murali for the discussions about Unity.", "mimetype": "text/plain", "start_char_idx": 3475, "end_char_idx": 4897, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f1febf98-47c6-4539-98f4-0802bb771b93": {"__data__": {"id_": "f1febf98-47c6-4539-98f4-0802bb771b93", "embedding": null, "metadata": {"page_label": "7", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "68a947d1-6c1e-42b1-8ba6-0a3f74cb3a88", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "e260ba705ce01296eff8aeb93187a2e0b23de94301eb9bc929872f60c4c362e3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46cd61fd-acd1-4c58-8f5c-043b4a8944a1", "node_type": "1", "metadata": {}, "hash": "c87e56e5b48764d30badf1f3ad2468b167c326512f7396b994990a2c3eae28c7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "REFERENCES\n[1] C. Cadena, L. Carlone, H. Carrillo, Y . Latif, D. Scaramuzza, J. Neira,\nI. Reid, and J. Leonard, \u201cPast, present, and future of simultaneous\nlocalization and mapping: Toward the robust-perception age,\u201d IEEE\nTrans. Robotics, vol. 32, no. 6, pp. 1309\u20131332, 2016, arxiv preprint:\n1606.05830, (pdf).\n[2] O. Enqvist, F. Kahl, and C. Olsson, \u201cNon-sequential structure from\nmotion,\u201d in Intl. Conf. on Computer Vision (ICCV) , 2011, pp. 264\u2013\n271.\n[3] T. Sch \u00a8ops, J. L. Sch \u00a8onberger, S. Galliani, T. Sattler, K. Schindler,\nM. Pollefeys, and A. Geiger, \u201cA multi-view stereo benchmark with\nhigh-resolution images and multi-camera videos,\u201d in Conference on\nComputer Vision and Pattern Recognition (CVPR) , 2017.\n[4] A. Garcia-Garcia, S. Orts-Escolano, S. Oprea, V . Villena-Martinez, and\nJ. Garc \u00b4\u0131a-Rodr\u00b4\u0131guez, \u201cA review on deep learning techniques applied\nto semantic segmentation,\u201d ArXiv Preprint: 1704.06857 , 2017.\n[5] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \u201cImageNet classi\ufb01cation\nwith deep convolutional neural networks,\u201d in Advances in Neural\nInformation Processing Systems (NIPS), ser. NIPS\u201912, 2012, pp. 1097\u2013\n1105.\n[6] J. Redmon and A. Farhadi, \u201cYOLO9000: Better, faster, stronger,\u201d in\nIEEE Conf. on Computer Vision and Pattern Recognition (CVPR) ,\n2017, pp. 6517\u20136525.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1292, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "46cd61fd-acd1-4c58-8f5c-043b4a8944a1": {"__data__": {"id_": "46cd61fd-acd1-4c58-8f5c-043b4a8944a1", "embedding": null, "metadata": {"page_label": "7", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "68a947d1-6c1e-42b1-8ba6-0a3f74cb3a88", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "e260ba705ce01296eff8aeb93187a2e0b23de94301eb9bc929872f60c4c362e3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1febf98-47c6-4539-98f4-0802bb771b93", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "added2637bf47ce4c6595b6a5e8ac13f1f348f7855d89e16d3862f4e1c6957ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f00e79f-fe2d-43f7-a8a1-d7ba41a578ce", "node_type": "1", "metadata": {}, "hash": "bb258230752fb5262478ce369460db8d29b44fbe75aac22597825d8ec8dd1304", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Villena-Martinez, and\nJ. Garc \u00b4\u0131a-Rodr\u00b4\u0131guez, \u201cA review on deep learning techniques applied\nto semantic segmentation,\u201d ArXiv Preprint: 1704.06857 , 2017.\n[5] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \u201cImageNet classi\ufb01cation\nwith deep convolutional neural networks,\u201d in Advances in Neural\nInformation Processing Systems (NIPS), ser. NIPS\u201912, 2012, pp. 1097\u2013\n1105.\n[6] J. Redmon and A. Farhadi, \u201cYOLO9000: Better, faster, stronger,\u201d in\nIEEE Conf. on Computer Vision and Pattern Recognition (CVPR) ,\n2017, pp. 6517\u20136525.\n[7] S. Ren, K. He, R. Girshick, and J. Sun, \u201cFaster R-CNN: Towards\nrealtime object detection with region proposal networks,\u201d in Advances\nin Neural Information Processing Systems (NIPS) , 2015, pp. 91\u201399.\n[8] K. He, G. Gkioxari, P. Dollar, and R. Girshick, \u201cMask R-CNN,\u201d in\nIntl. Conf. on Computer Vision (ICCV) , 2017, pp. 2980\u20132988.\n[9] R. Hu, P. Dollar, and K. He, \u201cLearning to segment every thing,\u201d in\nIntl. Conf. on Computer Vision (ICCV) , 2017, pp. 4233\u20134241.\n[10] V . Badrinarayanan, A. Kendall, and R. Cipolla, \u201cSegNet: A deep\nconvolutional encoder-decoder architecture for image segmentation,\u201d\nIEEE Trans. Pattern Anal. Machine Intell. , 2017.\n[11] S. Y .-Z. Bao and S. Savarese, \u201cSemantic structure from motion,\u201d in\nIEEE Conf. on Computer Vision and Pattern Recognition (CVPR) ,\n2011.", "mimetype": "text/plain", "start_char_idx": 770, "end_char_idx": 2087, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0f00e79f-fe2d-43f7-a8a1-d7ba41a578ce": {"__data__": {"id_": "0f00e79f-fe2d-43f7-a8a1-d7ba41a578ce", "embedding": null, "metadata": {"page_label": "7", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "68a947d1-6c1e-42b1-8ba6-0a3f74cb3a88", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "e260ba705ce01296eff8aeb93187a2e0b23de94301eb9bc929872f60c4c362e3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "46cd61fd-acd1-4c58-8f5c-043b4a8944a1", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "5c93c87533c401629784bca1baa2ad6969b788666f8c6b8cc711c5fba6a3da21", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "817c17f0-b69c-4431-92bc-6d0592ed8b2d", "node_type": "1", "metadata": {}, "hash": "1d8d0477ca83aaa7f1aa5db7b703a28f781b47d642dfcbedfb6d8c010018cb50", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conf. on Computer Vision (ICCV) , 2017, pp. 2980\u20132988.\n[9] R. Hu, P. Dollar, and K. He, \u201cLearning to segment every thing,\u201d in\nIntl. Conf. on Computer Vision (ICCV) , 2017, pp. 4233\u20134241.\n[10] V . Badrinarayanan, A. Kendall, and R. Cipolla, \u201cSegNet: A deep\nconvolutional encoder-decoder architecture for image segmentation,\u201d\nIEEE Trans. Pattern Anal. Machine Intell. , 2017.\n[11] S. Y .-Z. Bao and S. Savarese, \u201cSemantic structure from motion,\u201d in\nIEEE Conf. on Computer Vision and Pattern Recognition (CVPR) ,\n2011.\n[12] S. Bowman, N. Atanasov, K. Daniilidis, and G. Pappas, \u201cProbabilistic\ndata association for semantic slam,\u201d in IEEE Intl. Conf. on Robotics\nand Automation (ICRA) , 2017, pp. 1722\u20131729.\n[13] T. Hackel, N. Savinov, L. Ladicky, J. D. Wegner, K. Schindler,\nand M. Pollefeys, \u201cSemantic3d.net: A new large-scale point cloud\nclassi\ufb01cation benchmark,\u201d arXiv preprint arXiv:1704.03847 , 2017.\n[14] M. Grinvald, F. Furrer, T. Novkovic, J. J. Chung, C. Cadena, R. Sieg-\nwart, and J. Nieto, \u201cV olumetric Instance-Aware Semantic Mapping and\n3D Object Discovery,\u201d IEEE Robotics and Automation Letters , vol. 4,\nno. 3, pp. 3037\u20133044, 2019.\n[15] L. Zheng, C. Zhu, J. Zhang, H. Zhao, H. Huang, M. Niessner, and\nK. Xu, \u201cActive scene understanding via online semantic reconstruc-\ntion,\u201d arXiv preprint:1906.07409, 2019.", "mimetype": "text/plain", "start_char_idx": 1572, "end_char_idx": 2891, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "817c17f0-b69c-4431-92bc-6d0592ed8b2d": {"__data__": {"id_": "817c17f0-b69c-4431-92bc-6d0592ed8b2d", "embedding": null, "metadata": {"page_label": "7", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "68a947d1-6c1e-42b1-8ba6-0a3f74cb3a88", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "e260ba705ce01296eff8aeb93187a2e0b23de94301eb9bc929872f60c4c362e3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0f00e79f-fe2d-43f7-a8a1-d7ba41a578ce", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "589da0f2e7c7c04d182ff824e03eece651a95153a7458a2e3994c5e32be09031", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab4867f3-5019-47e8-b6eb-d6c446d8f28c", "node_type": "1", "metadata": {}, "hash": "52c8859cd66942583ee6ab3680161cd5677eb147ca230c7874b7805f4bba4c75", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[14] M. Grinvald, F. Furrer, T. Novkovic, J. J. Chung, C. Cadena, R. Sieg-\nwart, and J. Nieto, \u201cV olumetric Instance-Aware Semantic Mapping and\n3D Object Discovery,\u201d IEEE Robotics and Automation Letters , vol. 4,\nno. 3, pp. 3037\u20133044, 2019.\n[15] L. Zheng, C. Zhu, J. Zhang, H. Zhao, H. Huang, M. Niessner, and\nK. Xu, \u201cActive scene understanding via online semantic reconstruc-\ntion,\u201d arXiv preprint:1906.07409, 2019.\n[16] R. F. Salas-Moreno, R. A. Newcombe, H. Strasdat, P. H. J. Kelly, and\nA. J. Davison, \u201cSLAM++: Simultaneous localisation and mapping at\nthe level of objects,\u201d in IEEE Conf. on Computer Vision and Pattern\nRecognition (CVPR), 2013.\n[17] J. McCormac, A. Handa, A. J. Davison, and S. Leutenegger, \u201cSeman-\nticFusion: Dense 3D Semantic Mapping with Convolutional Neural\nNetworks,\u201d in IEEE Intl. Conf. on Robotics and Automation (ICRA) ,\n2017.\n[18] T. Whelan, S. Leutenegger, R. Salas-Moreno, B. Glocker, and A. Davi-\nson, \u201cElasticFusion: Dense SLAM without a pose graph,\u201d in Robotics:\nScience and Systems (RSS) , 2015.\n[19] M. Burri, J. Nikolic, P. Gohl, T. Schneider, J. Rehder, S. Omari,\nM. Achtelik, and R. Siegwart, \u201cThe EuRoC micro aerial vehicle\ndatasets,\u201d Intl. J. of Robotics Research , 2016.\n[20] R. Sayre-McCord, W. Guerra, A. Antonini, J. Arneberg, A. Brown,\nG. Cavalheiro, Y .", "mimetype": "text/plain", "start_char_idx": 2475, "end_char_idx": 3777, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ab4867f3-5019-47e8-b6eb-d6c446d8f28c": {"__data__": {"id_": "ab4867f3-5019-47e8-b6eb-d6c446d8f28c", "embedding": null, "metadata": {"page_label": "7", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "68a947d1-6c1e-42b1-8ba6-0a3f74cb3a88", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "e260ba705ce01296eff8aeb93187a2e0b23de94301eb9bc929872f60c4c362e3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "817c17f0-b69c-4431-92bc-6d0592ed8b2d", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "de9e882c20ae59bbabc87bffb7176d5d88ab40af52df4c54655374349e9c7bb9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b1500f67-b7c2-4e4e-aa74-6ddd5e568677", "node_type": "1", "metadata": {}, "hash": "10b31dfff8a2566aa648b4795637092b117e914b24f8655221e6eaa7fe98aaf5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conf. on Robotics and Automation (ICRA) ,\n2017.\n[18] T. Whelan, S. Leutenegger, R. Salas-Moreno, B. Glocker, and A. Davi-\nson, \u201cElasticFusion: Dense SLAM without a pose graph,\u201d in Robotics:\nScience and Systems (RSS) , 2015.\n[19] M. Burri, J. Nikolic, P. Gohl, T. Schneider, J. Rehder, S. Omari,\nM. Achtelik, and R. Siegwart, \u201cThe EuRoC micro aerial vehicle\ndatasets,\u201d Intl. J. of Robotics Research , 2016.\n[20] R. Sayre-McCord, W. Guerra, A. Antonini, J. Arneberg, A. Brown,\nG. Cavalheiro, Y . Fang, A. Gorodetsky, D. McCoy, S. Quilter, F. Ri-\nether, E. Tal, Y . Terzioglu, L. Carlone, and S. Karaman, \u201cVisual-inertial\nnavigation algorithm development using photorealistic camera simu-\nlation in the loop,\u201d in IEEE Intl. Conf. on Robotics and Automation\n(ICRA), 2018, (pdf) (code).\n[21] W. Guerra, E. Tal, V . Murali, G. Ryou, and S. Karaman, \u201cFlightGog-\ngles: Photorealistic sensor simulation for perception-driven robotics\nusing photogrammetry and virtual reality,\u201d in arXiv preprint:\n1905.11377, 2019.\n[22] R. Mur-Artal, J. Montiel, and J. Tard \u00b4os, \u201cORB-SLAM: A versatile and\naccurate monocular SLAM system,\u201d IEEE Trans. Robotics , vol. 31,\nno. 5, pp. 1147\u20131163, 2015.\n[23] J. Engel, V . Koltun, and D. Cremers, \u201cDirect sparse odometry,\u201d IEEE\nTrans. Pattern Anal. Machine Intell. , 2018.", "mimetype": "text/plain", "start_char_idx": 3284, "end_char_idx": 4575, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b1500f67-b7c2-4e4e-aa74-6ddd5e568677": {"__data__": {"id_": "b1500f67-b7c2-4e4e-aa74-6ddd5e568677", "embedding": null, "metadata": {"page_label": "7", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "68a947d1-6c1e-42b1-8ba6-0a3f74cb3a88", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "e260ba705ce01296eff8aeb93187a2e0b23de94301eb9bc929872f60c4c362e3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab4867f3-5019-47e8-b6eb-d6c446d8f28c", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "f3101af0bf6f8007ae3df94f530badc7a38d1e181f4e85ef5606e3814b87bf0a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb19e0e3-4ab1-4f7e-b25e-08920cfdb93e", "node_type": "1", "metadata": {}, "hash": "4767d4488d034b7dc4285051a2837d28c3c8b42cf85443ece2e988dd23b74d2a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[21] W. Guerra, E. Tal, V . Murali, G. Ryou, and S. Karaman, \u201cFlightGog-\ngles: Photorealistic sensor simulation for perception-driven robotics\nusing photogrammetry and virtual reality,\u201d in arXiv preprint:\n1905.11377, 2019.\n[22] R. Mur-Artal, J. Montiel, and J. Tard \u00b4os, \u201cORB-SLAM: A versatile and\naccurate monocular SLAM system,\u201d IEEE Trans. Robotics , vol. 31,\nno. 5, pp. 1147\u20131163, 2015.\n[23] J. Engel, V . Koltun, and D. Cremers, \u201cDirect sparse odometry,\u201d IEEE\nTrans. Pattern Anal. Machine Intell. , 2018.\n[24] T. Qin, P. Li, and S. Shen, \u201cVins-mono: A robust and versatile monoc-\nular visual-inertial state estimator,\u201d IEEE Transactions on Robotics ,\nvol. 34, no. 4, pp. 1004\u20131020, 2018.\n[25] T. Qin, J. Pan, S. Cao, and S. Shen, \u201cA general optimization-based\nframework for local odometry estimation with multiple sensors,\u201d arXiv\npreprint: 1901.03638, 2019.\n[26] T. Schneider, M. T. Dymczyk, M. Fehr, K. Egger, S. Lynen,\nI. Gilitschenski, and R. Siegwart, \u201cmaplab: An open framework for\nresearch in visual-inertial mapping and localization,\u201d IEEE Robotics\nand Automation Letters , 2018.\n[27] H. Oleynikova, Z. Taylor, M. Fehr, R. Siegwart, and J. Nieto,\n\u201cV oxblox: Incremental 3d euclidean signed distance \ufb01elds for on-\nboard mav planning,\u201d in IEEE/RSJ Intl. Conf. on Intelligent Robots\nand Systems (IROS) . IEEE, 2017, pp. 1366\u20131373.", "mimetype": "text/plain", "start_char_idx": 4066, "end_char_idx": 5405, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fb19e0e3-4ab1-4f7e-b25e-08920cfdb93e": {"__data__": {"id_": "fb19e0e3-4ab1-4f7e-b25e-08920cfdb93e", "embedding": null, "metadata": {"page_label": "7", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "68a947d1-6c1e-42b1-8ba6-0a3f74cb3a88", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "e260ba705ce01296eff8aeb93187a2e0b23de94301eb9bc929872f60c4c362e3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b1500f67-b7c2-4e4e-aa74-6ddd5e568677", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "b75d4f9466e9955d1a59f8cc3b7a1820cdab51630fe94e3cd266d4f6f28aafc1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "06335322-2d52-4e85-8b4e-e1932f57aa8a", "node_type": "1", "metadata": {}, "hash": "49ac83117fd3e0cd550d2b8527d452604b6e6bb4bd3fc0660c1bb4405659de56", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[26] T. Schneider, M. T. Dymczyk, M. Fehr, K. Egger, S. Lynen,\nI. Gilitschenski, and R. Siegwart, \u201cmaplab: An open framework for\nresearch in visual-inertial mapping and localization,\u201d IEEE Robotics\nand Automation Letters , 2018.\n[27] H. Oleynikova, Z. Taylor, M. Fehr, R. Siegwart, and J. Nieto,\n\u201cV oxblox: Incremental 3d euclidean signed distance \ufb01elds for on-\nboard mav planning,\u201d in IEEE/RSJ Intl. Conf. on Intelligent Robots\nand Systems (IROS) . IEEE, 2017, pp. 1366\u20131373.\n[28] M. Runz, M. Buf\ufb01er, and L. Agapito, \u201cMaskfusion: Real-time recogni-\ntion, tracking and reconstruction of multiple moving objects,\u201d in IEEE\nInternational Symposium on Mixed and Augmented Reality (ISMAR) .\nIEEE, 2018, pp. 10\u201320.\n[29] M. Keller, D. Le\ufb02och, M. Lambers, S. Izadi, T. Weyrich, and A. Kolb,\n\u201cReal-time 3d reconstruction in dynamic scenes using point-based\nfusion,\u201d in Intl. Conf. on 3D Vision (3DV) , 2013.\n[30] R. Dub \u00b4e, A. Cramariuc, D. Dugas, J. Nieto, R. Siegwart, and C. Ca-\ndena, \u201cSegMap: 3d segment mapping using data-driven descriptors,\u201d\nin Robotics: Science and Systems (RSS) , 2018.\n[31] J. Dong, X. Fei, and S. Soatto, \u201cVisual-inertial-semantic scene repre-\nsentation for 3D object detection,\u201d 2017.\n[32] G. J. Brostow, J. Shotton, J. Fauqueur, and R. Cipolla, \u201cSegmentation\nand recognition using structure from motion point clouds,\u201d in Euro-\npean Conf.", "mimetype": "text/plain", "start_char_idx": 4929, "end_char_idx": 6286, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "06335322-2d52-4e85-8b4e-e1932f57aa8a": {"__data__": {"id_": "06335322-2d52-4e85-8b4e-e1932f57aa8a", "embedding": null, "metadata": {"page_label": "7", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "68a947d1-6c1e-42b1-8ba6-0a3f74cb3a88", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "e260ba705ce01296eff8aeb93187a2e0b23de94301eb9bc929872f60c4c362e3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb19e0e3-4ab1-4f7e-b25e-08920cfdb93e", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "73f921a1102b5278ab48fbd35dc92a0050134d4dff808f55ed8a3feccffdf1f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c13c5070-f277-4a55-aef1-1ba5e14ad10c", "node_type": "1", "metadata": {}, "hash": "fc2a5a8d9e7c5edf21603a3b40554f9eaad64f55dd8a14451e004e460d3de423", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conf. on 3D Vision (3DV) , 2013.\n[30] R. Dub \u00b4e, A. Cramariuc, D. Dugas, J. Nieto, R. Siegwart, and C. Ca-\ndena, \u201cSegMap: 3d segment mapping using data-driven descriptors,\u201d\nin Robotics: Science and Systems (RSS) , 2018.\n[31] J. Dong, X. Fei, and S. Soatto, \u201cVisual-inertial-semantic scene repre-\nsentation for 3D object detection,\u201d 2017.\n[32] G. J. Brostow, J. Shotton, J. Fauqueur, and R. Cipolla, \u201cSegmentation\nand recognition using structure from motion point clouds,\u201d in Euro-\npean Conf. on Computer Vision (ECCV) , 2008, pp. 44\u201357.\n[33] K. Tateno, F. Tombari, and N. Navab, \u201cReal-time and scalable in-\ncremental segmentation on dense slam,\u201d in IEEE/RSJ Intl. Conf. on\nIntelligent Robots and Systems (IROS) , 2015, pp. 4465\u20134472.\n[34] C. Li, H. Xiao, K. Tateno, F. Tombari, N. Navab, and G. D. Hager,\n\u201cIncremental scene understanding on dense SLAM,\u201d in IEEE/RSJ Intl.\nConf. on Intelligent Robots and Systems (IROS) , 2016, pp. 574\u2013581.\n[35] J. McCormac, R. Clark, M. Bloesch, A. J. Davison, and S. Leuteneg-\nger, \u201cFusion++: V olumetric object-level SLAM,\u201d in Intl. Conf. on 3D\nVision (3DV), 2018, pp. 32\u201341.\n[36] M. R \u00a8unz and L. Agapito, \u201cCo-fusion: Real-time segmentation, tracking\nand fusion of multiple objects,\u201d in IEEE Intl. Conf. on Robotics and\nAutomation (ICRA). IEEE, 2017, pp. 4471\u20134478.", "mimetype": "text/plain", "start_char_idx": 5795, "end_char_idx": 7097, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c13c5070-f277-4a55-aef1-1ba5e14ad10c": {"__data__": {"id_": "c13c5070-f277-4a55-aef1-1ba5e14ad10c", "embedding": null, "metadata": {"page_label": "7", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "68a947d1-6c1e-42b1-8ba6-0a3f74cb3a88", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "e260ba705ce01296eff8aeb93187a2e0b23de94301eb9bc929872f60c4c362e3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "06335322-2d52-4e85-8b4e-e1932f57aa8a", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "e9a29f958eaa761d83815722e1571888ec3da1b9e3655f67867b9713ebe976bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "15ea3d6f-be19-4566-885a-f9acd3a9953a", "node_type": "1", "metadata": {}, "hash": "bae4cfb083bedf4a6ace45160f7b392f08ae10e4ae43f42784ffbe8d734ab18b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conf. on Intelligent Robots and Systems (IROS) , 2016, pp. 574\u2013581.\n[35] J. McCormac, R. Clark, M. Bloesch, A. J. Davison, and S. Leuteneg-\nger, \u201cFusion++: V olumetric object-level SLAM,\u201d in Intl. Conf. on 3D\nVision (3DV), 2018, pp. 32\u201341.\n[36] M. R \u00a8unz and L. Agapito, \u201cCo-fusion: Real-time segmentation, tracking\nand fusion of multiple objects,\u201d in IEEE Intl. Conf. on Robotics and\nAutomation (ICRA). IEEE, 2017, pp. 4471\u20134478.\n[37] B. Xu, W. Li, D. Tzoumanikas, M. Bloesch, A. Davison, and\nS. Leutenegger, \u201cMID-Fusion: Octree-based object-level multi-\ninstance dynamic slam,\u201d 2019, pp. 5231\u20135237.\n[38] J. Wald, K. Tateno, J. Sturm, N. Navab, and F. Tombari, \u201cReal-time\nfully incremental scene understanding on mobile platforms,\u201d IEEE\nRobotics and Automation Letters , vol. 3, no. 4, pp. 3402\u20133409, 2018.\n[39] G. Narita, T. Seno, T. Ishikawa, and Y . Kaji, \u201cPanopticfusion: Online\nvolumetric semantic mapping at the level of stuff and things,\u201d arxiv\npreprint: 1903.01177, 2019.\n[40] K. Tateno, F. Tombari, I. Laina, and N. Navab, \u201cCNN-SLAM: Real-\ntime dense monocular slam with learned depth prediction,\u201d in IEEE\nConf. on Computer Vision and Pattern Recognition (CVPR) , 2017.\n[41] K.-N. Lianos, J. L. Sch \u00a8onberger, M. Pollefeys, and T. Sattler, \u201cVso:\nVisual semantic odometry,\u201d in European Conf.", "mimetype": "text/plain", "start_char_idx": 6667, "end_char_idx": 7967, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "15ea3d6f-be19-4566-885a-f9acd3a9953a": {"__data__": {"id_": "15ea3d6f-be19-4566-885a-f9acd3a9953a", "embedding": null, "metadata": {"page_label": "7", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "68a947d1-6c1e-42b1-8ba6-0a3f74cb3a88", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "e260ba705ce01296eff8aeb93187a2e0b23de94301eb9bc929872f60c4c362e3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c13c5070-f277-4a55-aef1-1ba5e14ad10c", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "222c4aafba881511a857720fe5cc21f2f74068f12483e00da0ae9896c93e070f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c725533-df8f-4e53-9353-9efca6ec2ae7", "node_type": "1", "metadata": {}, "hash": "2f8edce0354908dc63a682a9a5a4ab23732c58d6d29d4b362bd226589de46903", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3, no. 4, pp. 3402\u20133409, 2018.\n[39] G. Narita, T. Seno, T. Ishikawa, and Y . Kaji, \u201cPanopticfusion: Online\nvolumetric semantic mapping at the level of stuff and things,\u201d arxiv\npreprint: 1903.01177, 2019.\n[40] K. Tateno, F. Tombari, I. Laina, and N. Navab, \u201cCNN-SLAM: Real-\ntime dense monocular slam with learned depth prediction,\u201d in IEEE\nConf. on Computer Vision and Pattern Recognition (CVPR) , 2017.\n[41] K.-N. Lianos, J. L. Sch \u00a8onberger, M. Pollefeys, and T. Sattler, \u201cVso:\nVisual semantic odometry,\u201d in European Conf. on Computer Vision\n(ECCV), 2018, pp. 246\u2013263.\n[42] M. Yokozuka, S. Oishi, S. Thompson, and A. Banno, \u201cVITAMIN-\nE: visual tracking and mapping with extremely dense feature points,\u201d\nCoRR, vol. abs/1904.10324, 2019.\n[43] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stach-\nniss, and J. Gall, \u201cSemanticKITTI: A Dataset for Semantic Scene\nUnderstanding of LiDAR Sequences,\u201d in Intl. Conf. on Computer\nVision (ICCV), 2019.\n[44] F. Dellaert et al., \u201cGeorgia Tech Smoothing And Mapping (GTSAM),\u201d\nhttps://gtsam.org/, 2019.\n[45] C. Forster, L. Carlone, F. Dellaert, and D. Scaramuzza, \u201cOn-manifold\npreintegration theory for fast and accurate visual-inertial navigation,\u201d\nIEEE Trans. Robotics , vol. 33, no. 1, pp.", "mimetype": "text/plain", "start_char_idx": 7444, "end_char_idx": 8687, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0c725533-df8f-4e53-9353-9efca6ec2ae7": {"__data__": {"id_": "0c725533-df8f-4e53-9353-9efca6ec2ae7", "embedding": null, "metadata": {"page_label": "7", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "68a947d1-6c1e-42b1-8ba6-0a3f74cb3a88", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "e260ba705ce01296eff8aeb93187a2e0b23de94301eb9bc929872f60c4c362e3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "15ea3d6f-be19-4566-885a-f9acd3a9953a", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "98a6db48afdd22fbc668c8d92dc5e0419d890c425b50301991c89ae59b691799", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "abs/1904.10324, 2019.\n[43] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stach-\nniss, and J. Gall, \u201cSemanticKITTI: A Dataset for Semantic Scene\nUnderstanding of LiDAR Sequences,\u201d in Intl. Conf. on Computer\nVision (ICCV), 2019.\n[44] F. Dellaert et al., \u201cGeorgia Tech Smoothing And Mapping (GTSAM),\u201d\nhttps://gtsam.org/, 2019.\n[45] C. Forster, L. Carlone, F. Dellaert, and D. Scaramuzza, \u201cOn-manifold\npreintegration theory for fast and accurate visual-inertial navigation,\u201d\nIEEE Trans. Robotics , vol. 33, no. 1, pp. 1\u201321, 2017, arxiv preprint:\n1512.02363, (pdf), technical report GT-IRIM-CP&R-2015-001.", "mimetype": "text/plain", "start_char_idx": 8159, "end_char_idx": 8774, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7409e5a2-b727-42de-864c-99591a88d124": {"__data__": {"id_": "7409e5a2-b727-42de-864c-99591a88d124", "embedding": null, "metadata": {"page_label": "8", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d21a27c0-c906-44c8-8ef0-37e5dcf082f6", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "eb886d818cd3a3ce579071c6b723d175cd0c154f9c2ac2333500a8ad92e4cc15", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "27cb569c-d400-46aa-8822-e79ad5cb9c3f", "node_type": "1", "metadata": {}, "hash": "42891db176507451eecf0bf44fa92849e1ed74f1d65dad0a814fa30aa2b49c85", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[46] J. G. Mangelson, D. Dominic, R. M. Eustice, and R. Vasudevan,\n\u201cPairwise consistent measurement set maximization for robust multi-\nrobot map merging,\u201d in IEEE Intl. Conf. on Robotics and Automation\n(ICRA), 2018, pp. 2916\u20132923.\n[47] A. Rosinol, T. Sattler, M. Pollefeys, and L. Carlone, \u201cIncremental\nVisual-Inertial 3D Mesh Generation with Structural Regularities,\u201d\nin IEEE Intl. Conf. on Robotics and Automation (ICRA) , 2019,\n(pdf), (web). [Online]. Available: https://www.mit.edu/%7Earosinol/\nresearch/struct3dmesh.html\n[48] W. N. Greene and N. Roy, \u201cFlame: Fast lightweight mesh estimation\nusing variational smoothing on delaunay graphs,\u201d in 2017 IEEE\nInternational Conference on Computer Vision (ICCV) . IEEE, 2017,\npp. 4696\u20134704.\n[49] L. Teixeira and M. Chli, \u201cReal-time mesh-based scene estimation for\naerial inspection,\u201d in IEEE/RSJ Intl. Conf. on Intelligent Robots and\nSystems (IROS). IEEE, 2016, pp. 4863\u20134869.\n[50] M. Quigley, K. Conley, B. Gerkey, J. Faust, T. Foote, J. Leibs,\nR. Wheeler, and A. Y . Ng, \u201cRos: an open-source robot operating\nsystem,\u201d in ICRA workshop on open source software , vol. 3, no. 3.2.\nKobe, Japan, 2009, p. 5.\n[51] J. Shi and C. Tomasi, \u201cGood features to track,\u201d in IEEE Conf. on\nComputer Vision and Pattern Recognition (CVPR) , 1994, pp. 593\u2013\n600.\n[52] J. Bouguet, \u201cPyramidal implementation of the Lucas Kanade feature\ntracker,\u201d 2000.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1377, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "27cb569c-d400-46aa-8822-e79ad5cb9c3f": {"__data__": {"id_": "27cb569c-d400-46aa-8822-e79ad5cb9c3f", "embedding": null, "metadata": {"page_label": "8", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d21a27c0-c906-44c8-8ef0-37e5dcf082f6", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "eb886d818cd3a3ce579071c6b723d175cd0c154f9c2ac2333500a8ad92e4cc15", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7409e5a2-b727-42de-864c-99591a88d124", "node_type": "1", "metadata": {"page_label": "8", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "fbc6268f4b46239cc044445df605206d13de58f1fcb908ce41017320cc6e7863", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5192829f-a132-4383-bf97-6f8da6112994", "node_type": "1", "metadata": {}, "hash": "f4052166f686f331b4721bd40bd05b6311a768350c4d146e3a01177f5ab90582", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conf. on Intelligent Robots and\nSystems (IROS). IEEE, 2016, pp. 4863\u20134869.\n[50] M. Quigley, K. Conley, B. Gerkey, J. Faust, T. Foote, J. Leibs,\nR. Wheeler, and A. Y . Ng, \u201cRos: an open-source robot operating\nsystem,\u201d in ICRA workshop on open source software , vol. 3, no. 3.2.\nKobe, Japan, 2009, p. 5.\n[51] J. Shi and C. Tomasi, \u201cGood features to track,\u201d in IEEE Conf. on\nComputer Vision and Pattern Recognition (CVPR) , 1994, pp. 593\u2013\n600.\n[52] J. Bouguet, \u201cPyramidal implementation of the Lucas Kanade feature\ntracker,\u201d 2000.\n[53] D. Nist \u00b4er, \u201cAn ef\ufb01cient solution to the \ufb01ve-point relative pose prob-\nlem,\u201d IEEE Trans. Pattern Anal. Machine Intell. , vol. 26, no. 6, pp.\n756\u2013770, 2004.\n[54] B. Horn, \u201cClosed-form solution of absolute orientation using unit\nquaternions,\u201d J. Opt. Soc. Amer., vol. 4, no. 4, pp. 629\u2013642, Apr 1987.\n[55] L. Kneip, M. Chli, and R. Siegwart, \u201cRobust real-time visual odometry\nwith a single camera and an IMU,\u201d in British Machine Vision Conf.\n(BMVC), 2011, pp. 16.1\u201316.11.\n[56] M. Kaess, H. Johannsson, R. Roberts, V . Ila, J. Leonard, and F. Del-\nlaert, \u201ciSAM2: Incremental smoothing and mapping using the Bayes\ntree,\u201d Intl. J. of Robotics Research , vol. 31, pp. 217\u2013236, Feb 2012.\n[57] F. Dellaert, \u201cFactor graphs and GTSAM: A hands-on introduction,\u201d\nGeorgia Institute of Technology, Tech. Rep.", "mimetype": "text/plain", "start_char_idx": 850, "end_char_idx": 2178, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5192829f-a132-4383-bf97-6f8da6112994": {"__data__": {"id_": "5192829f-a132-4383-bf97-6f8da6112994", "embedding": null, "metadata": {"page_label": "8", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d21a27c0-c906-44c8-8ef0-37e5dcf082f6", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "eb886d818cd3a3ce579071c6b723d175cd0c154f9c2ac2333500a8ad92e4cc15", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "27cb569c-d400-46aa-8822-e79ad5cb9c3f", "node_type": "1", "metadata": {"page_label": "8", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "d0bc9346fc891ca7e85527f810f3e6e4e96d7c77464fe9fcc4dcede61d9fb05c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6d305b6f-e266-4661-8b27-23c114a1e436", "node_type": "1", "metadata": {}, "hash": "f372334e66038c1c721055eb0aea0b55327bfc7861dcfee14fef711fde53da5b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4, no. 4, pp. 629\u2013642, Apr 1987.\n[55] L. Kneip, M. Chli, and R. Siegwart, \u201cRobust real-time visual odometry\nwith a single camera and an IMU,\u201d in British Machine Vision Conf.\n(BMVC), 2011, pp. 16.1\u201316.11.\n[56] M. Kaess, H. Johannsson, R. Roberts, V . Ila, J. Leonard, and F. Del-\nlaert, \u201ciSAM2: Incremental smoothing and mapping using the Bayes\ntree,\u201d Intl. J. of Robotics Research , vol. 31, pp. 217\u2013236, Feb 2012.\n[57] F. Dellaert, \u201cFactor graphs and GTSAM: A hands-on introduction,\u201d\nGeorgia Institute of Technology, Tech. Rep. GT-RIM-CP&R-2012-\n002, September 2012.\n[58] R. I. Hartley and A. Zisserman, Multiple View Geometry in Computer\nVision, 2nd ed. Cambridge University Press, 2004.\n[59] L. Carlone, Z. Kira, C. Beall, V . Indelman, and F. Dellaert, \u201cElim-\ninating conditionally independent sets in factor graphs: A unifying\nperspective based on smart factors,\u201d in IEEE Intl. Conf. on Robotics\nand Automation (ICRA) , 2014, pp. 4290\u20134297.\n[60] D. G \u00b4alvez-L\u00b4opez and J. D. Tard\u00b4os, \u201cBags of binary words for fast place\nrecognition in image sequences,\u201d IEEE Transactions on Robotics ,\nvol. 28, no. 5, pp. 1188\u20131197, October 2012.\n[61] B. Pattabiraman, M. M. A. Patwary, A. H. Gebremedhin, W. K. Liao,\nand A. Choudhary, \u201cFast algorithms for the maximum clique prob-\nlem on massive graphs with applications to overlapping community\ndetection,\u201d Internet Mathematics, vol. 11, no. 4-5, pp.", "mimetype": "text/plain", "start_char_idx": 1650, "end_char_idx": 3041, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6d305b6f-e266-4661-8b27-23c114a1e436": {"__data__": {"id_": "6d305b6f-e266-4661-8b27-23c114a1e436", "embedding": null, "metadata": {"page_label": "8", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d21a27c0-c906-44c8-8ef0-37e5dcf082f6", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "eb886d818cd3a3ce579071c6b723d175cd0c154f9c2ac2333500a8ad92e4cc15", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5192829f-a132-4383-bf97-6f8da6112994", "node_type": "1", "metadata": {"page_label": "8", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "6c5d90055370714e01b000f296485e0f8fd5d7e441b1bbb50669a4dd6e1ed1c4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dc2794e1-23f2-4790-8646-bb201daef271", "node_type": "1", "metadata": {}, "hash": "4afb8a83c6d8e0d8c2f9f3f5bda2514feaca3aa297b216a8a3c528e6bd752a04", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conf. on Robotics\nand Automation (ICRA) , 2014, pp. 4290\u20134297.\n[60] D. G \u00b4alvez-L\u00b4opez and J. D. Tard\u00b4os, \u201cBags of binary words for fast place\nrecognition in image sequences,\u201d IEEE Transactions on Robotics ,\nvol. 28, no. 5, pp. 1188\u20131197, October 2012.\n[61] B. Pattabiraman, M. M. A. Patwary, A. H. Gebremedhin, W. K. Liao,\nand A. Choudhary, \u201cFast algorithms for the maximum clique prob-\nlem on massive graphs with applications to overlapping community\ndetection,\u201d Internet Mathematics, vol. 11, no. 4-5, pp. 421\u2013448, 2015.\n[62] H. H. Hirschm \u00a8uller, \u201cStereo processing by semiglobal matching and\nmutual information,\u201d IEEE Trans. Pattern Anal. Machine Intell. ,\nvol. 30, no. 2, pp. 328\u2013341, 2008.\n[63] W. Lorensen and H. Cline, \u201cMarching cubes: A high resolution 3d\nsurface construction algorithm,\u201d in SIGGRAPH, 1987, pp. 163\u2013169.\n[64] H. Lang, Y . Yuhui, G. Jianyuan, Z. Chao, C. Xilin, and W. Jingdong,\n\u201cInterlaced sparse self-attention for semantic segmentation,\u201d arXiv\npreprint arXiv:1907.12273, 2019.\n[65] L. Zhang, X. Li, A. Arnab, K. Yang, Y . Tong, and P. H. Torr, \u201cDual\ngraph convolutional network for semantic segmentation,\u201d in British\nMachine Vision Conference, 2019.\n[66] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille,\n\u201cDeeplab: Semantic image segmentation with deep convolutional nets,\natrous convolution, and fully connected crfs,\u201d IEEE Trans. Pattern\nAnal.", "mimetype": "text/plain", "start_char_idx": 2533, "end_char_idx": 3926, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dc2794e1-23f2-4790-8646-bb201daef271": {"__data__": {"id_": "dc2794e1-23f2-4790-8646-bb201daef271", "embedding": null, "metadata": {"page_label": "8", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d21a27c0-c906-44c8-8ef0-37e5dcf082f6", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "eb886d818cd3a3ce579071c6b723d175cd0c154f9c2ac2333500a8ad92e4cc15", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6d305b6f-e266-4661-8b27-23c114a1e436", "node_type": "1", "metadata": {"page_label": "8", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "ce28613217c97cee512f3416283048993dd79de1f530d6538b37957874439c03", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3437b056-56d5-4e36-a6e4-fe449f47172c", "node_type": "1", "metadata": {}, "hash": "d632514a6d06cfff9409a50394aee0479a623438f22758e7f4165f7d6bf6a4c1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "163\u2013169.\n[64] H. Lang, Y . Yuhui, G. Jianyuan, Z. Chao, C. Xilin, and W. Jingdong,\n\u201cInterlaced sparse self-attention for semantic segmentation,\u201d arXiv\npreprint arXiv:1907.12273, 2019.\n[65] L. Zhang, X. Li, A. Arnab, K. Yang, Y . Tong, and P. H. Torr, \u201cDual\ngraph convolutional network for semantic segmentation,\u201d in British\nMachine Vision Conference, 2019.\n[66] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille,\n\u201cDeeplab: Semantic image segmentation with deep convolutional nets,\natrous convolution, and fully connected crfs,\u201d IEEE Trans. Pattern\nAnal. Machine Intell. , vol. 40, no. 4, pp. 834\u2013848, 2017.\n[67] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, \u201cPyramid scene parsing\nnetwork,\u201d in IEEE Conf. on Computer Vision and Pattern Recognition\n(CVPR), 2017, pp. 2881\u20132890.\n[68] G. Yang, H. Zhao, J. Shi, Z. Deng, and J. Jia, \u201cSegstereo: Exploiting\nsemantic information for disparity estimation,\u201d in Proceedings of the\nEuropean Conference on Computer Vision (ECCV) , 2018, pp. 636\u2013\n651.\n[69] A. Paszke, A. Chaurasia, S. Kim, and E. Culurciello, \u201cEnet: A\ndeep neural network architecture for real-time semantic segmentation,\u201d\narXiv preprint arXiv:1606.02147 , 2016.\n[70] S. Hu and L. Carlone, \u201cAccelerated inference in Markov Random\nFields via smooth Riemannian optimization,\u201d IEEE Robotics and\nAutomation Letters (RA-L) , 2019, extended ArXiv version: (pdf).", "mimetype": "text/plain", "start_char_idx": 3355, "end_char_idx": 4733, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3437b056-56d5-4e36-a6e4-fe449f47172c": {"__data__": {"id_": "3437b056-56d5-4e36-a6e4-fe449f47172c", "embedding": null, "metadata": {"page_label": "8", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d21a27c0-c906-44c8-8ef0-37e5dcf082f6", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "eb886d818cd3a3ce579071c6b723d175cd0c154f9c2ac2333500a8ad92e4cc15", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc2794e1-23f2-4790-8646-bb201daef271", "node_type": "1", "metadata": {"page_label": "8", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "c8702a50ca2d417f44ee7b7fb90642efb298c630695a43a4f7e05f57ac259f98", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "924d7bbb-c072-42cb-bc72-b9cb9bc4e75b", "node_type": "1", "metadata": {}, "hash": "5fe1ef45e89f05b2fe2c2516e809a26b9c0755d600fb9351460135fa24e520a3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2881\u20132890.\n[68] G. Yang, H. Zhao, J. Shi, Z. Deng, and J. Jia, \u201cSegstereo: Exploiting\nsemantic information for disparity estimation,\u201d in Proceedings of the\nEuropean Conference on Computer Vision (ECCV) , 2018, pp. 636\u2013\n651.\n[69] A. Paszke, A. Chaurasia, S. Kim, and E. Culurciello, \u201cEnet: A\ndeep neural network architecture for real-time semantic segmentation,\u201d\narXiv preprint arXiv:1606.02147 , 2016.\n[70] S. Hu and L. Carlone, \u201cAccelerated inference in Markov Random\nFields via smooth Riemannian optimization,\u201d IEEE Robotics and\nAutomation Letters (RA-L) , 2019, extended ArXiv version: (pdf).\n[71] M. Grupp, \u201cevo: Python package for the evaluation of odometry and\nslam.\u201d https://github.com/MichaelGrupp/evo, 2017.\n[72] Q.-Y . Zhou, J. Park, and V . Koltun, \u201cOpen3D: A modern library for\n3D data processing,\u201d arXiv:1801.09847, 2018.\n[73] S. Leutenegger, P. Furgale, V . Rabaud, M. Chli, K. Konolige, and\nR. Siegwart, \u201cKeyframe-based visual-inertial slam using nonlinear\noptimization,\u201d in Robotics: Science and Systems (RSS) , 2013.\n[74] A. Mourikis and S. Roumeliotis, \u201cA multi-state constraint Kalman\n\ufb01lter for vision-aided inertial navigation,\u201d in IEEE Intl. Conf. on\nRobotics and Automation (ICRA) , April 2007, pp. 3565\u20133572.\n[75] M. Bloesch, S. Omari, M. Hutter, and R. Siegwart, \u201cRobust visual\ninertial odometry using a direct EKF-based approach,\u201d in IEEE/RSJ\nIntl. Conf. on Intelligent Robots and Systems (IROS) .", "mimetype": "text/plain", "start_char_idx": 4138, "end_char_idx": 5560, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "924d7bbb-c072-42cb-bc72-b9cb9bc4e75b": {"__data__": {"id_": "924d7bbb-c072-42cb-bc72-b9cb9bc4e75b", "embedding": null, "metadata": {"page_label": "8", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d21a27c0-c906-44c8-8ef0-37e5dcf082f6", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "eb886d818cd3a3ce579071c6b723d175cd0c154f9c2ac2333500a8ad92e4cc15", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3437b056-56d5-4e36-a6e4-fe449f47172c", "node_type": "1", "metadata": {"page_label": "8", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "3ce86f127e4bf1c914920feaac2ad51fb1cbbb52f733c695e827d96a7c20182b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ce6d2624-117c-45c1-811a-5b4c9eca1af8", "node_type": "1", "metadata": {}, "hash": "25bc2c0ac6a46aa0a70fa2f714036c579c72f8cf7d80f31ae8d2d3289b86b864", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Rabaud, M. Chli, K. Konolige, and\nR. Siegwart, \u201cKeyframe-based visual-inertial slam using nonlinear\noptimization,\u201d in Robotics: Science and Systems (RSS) , 2013.\n[74] A. Mourikis and S. Roumeliotis, \u201cA multi-state constraint Kalman\n\ufb01lter for vision-aided inertial navigation,\u201d in IEEE Intl. Conf. on\nRobotics and Automation (ICRA) , April 2007, pp. 3565\u20133572.\n[75] M. Bloesch, S. Omari, M. Hutter, and R. Siegwart, \u201cRobust visual\ninertial odometry using a direct EKF-based approach,\u201d in IEEE/RSJ\nIntl. Conf. on Intelligent Robots and Systems (IROS) . IEEE, 2015.\n[76] C. Forster, L. Carlone, F. Dellaert, and D. Scaramuzza, \u201cIMU preinte-\ngration on manifold for ef\ufb01cient visual-inertial maximum-a-posteriori\nestimation,\u201d in Robotics: Science and Systems (RSS) , 2015, accepted\nas oral presentation (acceptance rate 4%) (pdf) (video) (supplemental\nmaterial: (pdf)).\n[77] J. Delmerico and D. Scaramuzza, \u201cA benchmark comparison of\nmonocular visual-inertial odometry algorithms for \ufb02ying robots,\u201d in\n2018 IEEE International Conference on Robotics and Automation\n(ICRA). IEEE, 2018, pp. 2502\u20132509.\n[78] A. Rosinol, \u201cDensifying Sparse VIO: a Mesh-based approach using\nStructural Regularities.\u201d Master\u2019s thesis, ETH Zurich, 2018.\n[79] P. J. Besl and N. D. McKay, \u201cA method for registration of 3-D shapes,\u201d\nIEEE Trans. Pattern Anal. Machine Intell. , vol. 14, no. 2, 1992.\n[80] Cloudcompare.org, \u201cCloudCompare - open source project,\u201d https://\nwww.cloudcompare.org, 2019.", "mimetype": "text/plain", "start_char_idx": 5010, "end_char_idx": 6473, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ce6d2624-117c-45c1-811a-5b4c9eca1af8": {"__data__": {"id_": "ce6d2624-117c-45c1-811a-5b4c9eca1af8", "embedding": null, "metadata": {"page_label": "8", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d21a27c0-c906-44c8-8ef0-37e5dcf082f6", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "eb886d818cd3a3ce579071c6b723d175cd0c154f9c2ac2333500a8ad92e4cc15", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "924d7bbb-c072-42cb-bc72-b9cb9bc4e75b", "node_type": "1", "metadata": {"page_label": "8", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}, "hash": "999c2a7e16ad6799ea2cf0afcb6e7e1c09fc1ef087f773dd8e7ea25e0b0033bc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[77] J. Delmerico and D. Scaramuzza, \u201cA benchmark comparison of\nmonocular visual-inertial odometry algorithms for \ufb02ying robots,\u201d in\n2018 IEEE International Conference on Robotics and Automation\n(ICRA). IEEE, 2018, pp. 2502\u20132509.\n[78] A. Rosinol, \u201cDensifying Sparse VIO: a Mesh-based approach using\nStructural Regularities.\u201d Master\u2019s thesis, ETH Zurich, 2018.\n[79] P. J. Besl and N. D. McKay, \u201cA method for registration of 3-D shapes,\u201d\nIEEE Trans. Pattern Anal. Machine Intell. , vol. 14, no. 2, 1992.\n[80] Cloudcompare.org, \u201cCloudCompare - open source project,\u201d https://\nwww.cloudcompare.org, 2019.\n[81] D. Wolf, J. Prankl, and M. Vincze, \u201cEnhancing semantic segmentation\nfor robotics: The power of 3-d entangled forests,\u201d IEEE Robotics and\nAutomation Letters, vol. 1, no. 1, pp. 49\u201356, 2015.", "mimetype": "text/plain", "start_char_idx": 5875, "end_char_idx": 6667, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"1690b49b-e337-4fb2-81ba-6e9d22e0f0d2": {"doc_hash": "324cab0abdc6e87fda21743f6af0c5a312647991563903049607e8c1626686fa", "ref_doc_id": "975f80d9-59dd-4926-a5da-0d4af8fa6e7a"}, "71330889-f270-4edf-84bf-46777db79be0": {"doc_hash": "7e978c0b8243a52b70d1005bfecafd1337b2c3214876e9a2f99ebd6822de7f32", "ref_doc_id": "975f80d9-59dd-4926-a5da-0d4af8fa6e7a"}, "fd1de8c7-6e7d-45b0-94dc-926acd2a9931": {"doc_hash": "014b015b9e3d29c5675534b01484e2fc08e39d138f50e502575c1e6ab54fdc73", "ref_doc_id": "975f80d9-59dd-4926-a5da-0d4af8fa6e7a"}, "4347790c-3c18-49a5-bb21-dc3b6d1cd3e7": {"doc_hash": "4081be7d8a28990842068fe7f20ed54cd7f3055e756e30733f5420ad659c187c", "ref_doc_id": "7d9b2c19-f852-466e-b181-07c2b7cb445a"}, "fe5ab592-b3c9-48b4-826f-85b97b7dac10": {"doc_hash": "858b3e1b86a4dfd471e42391ace13c3a1e59c78ac181c4d0a744cb87d25a524b", "ref_doc_id": "7d9b2c19-f852-466e-b181-07c2b7cb445a"}, "63d7c5e5-5ea1-4d52-b0fa-07f6a29d7ff9": {"doc_hash": "182fb71db5b86b0f5cb5cb1a87f8e9ba8d1b5d33862e7b4f5e969cf33269bf9f", "ref_doc_id": "7d9b2c19-f852-466e-b181-07c2b7cb445a"}, "52a62b94-9292-4ded-9575-a06d26587a42": {"doc_hash": "504f2ba086e63eb40578d052b741da117fc07757f7b9d1ea698a9f0d035f4379", "ref_doc_id": "7d9b2c19-f852-466e-b181-07c2b7cb445a"}, "98430460-6eed-4a39-a767-90479f1408c3": {"doc_hash": "188b60cd1b18ca7d7c881ad30f8b64b1f9818e7bad172b8f40eae0b31ea11a2b", "ref_doc_id": "7d9b2c19-f852-466e-b181-07c2b7cb445a"}, "0f3b799e-b065-416f-818e-2e44ac00b6aa": {"doc_hash": "c23932198556b67285500e9b7e03c89da21842c868c90a9793ed638ad59d9bca", "ref_doc_id": "7d9b2c19-f852-466e-b181-07c2b7cb445a"}, "cff62621-5066-43e2-85ce-b121d0204585": {"doc_hash": "2f700818d10b9452b44e66690869efda70a0848f9caf74899ce6bb40cf7aedf4", "ref_doc_id": "e78cd701-1c80-4882-82f0-232fb546ed58"}, "903da76a-7bb8-423d-afbc-d54db070cf5d": {"doc_hash": "78559c5643ecd5f868001300b8c400224ce47a0b97e6072794f60f5e6f4a7eb0", "ref_doc_id": "e78cd701-1c80-4882-82f0-232fb546ed58"}, "c1eea650-23f6-45f2-a27c-68b98e637cb5": {"doc_hash": "18524ad761eb2906dd6429094a9086e60832d1fa72fd68a5690047e3d69bbc81", "ref_doc_id": "e78cd701-1c80-4882-82f0-232fb546ed58"}, "2a76b8b0-3a7b-4ed0-94f2-4fd315547377": {"doc_hash": "034b3db3f0ae3b6fb82e167d2c7b17535ce4b92649c6ff441bfdf8578701df1c", "ref_doc_id": "d9e034f3-f435-456d-9c7d-9a03358f3a73"}, "253e50c5-3d56-4612-96b0-88d48e5492b7": {"doc_hash": "98723581374a46ecbcfd26691dae7e2cbba2ea646a845d93c3da18a21d6c8c97", "ref_doc_id": "d9e034f3-f435-456d-9c7d-9a03358f3a73"}, "4faa2f83-321e-41ba-86f3-ea7411c93717": {"doc_hash": "b88cc3f5c15f3686e4071dee40f605f3b23f13b11c73f95278544e0cd04348a5", "ref_doc_id": "d9e034f3-f435-456d-9c7d-9a03358f3a73"}, "2a3a7c67-571b-4d7f-8f6c-3538fb7f7a53": {"doc_hash": "d26800b103430044266716e489db2b83e71a58f826caa7a7c48184c983ef5345", "ref_doc_id": "d9e034f3-f435-456d-9c7d-9a03358f3a73"}, "92988799-7642-4005-9b93-d072a5b82633": {"doc_hash": "ed398b91d7b03035a4e7d882d0565b2b8385e04f4142b8802315afea865ecc9c", "ref_doc_id": "d9e034f3-f435-456d-9c7d-9a03358f3a73"}, "0240a1ae-7fd4-49aa-b10d-8edf42ba1e50": {"doc_hash": "4e0df50659695e8930644ddde6e1d117d2c4d82388c0ca1527b8836ce79f4796", "ref_doc_id": "2c966173-c71d-4a61-8c00-afcb6570a42a"}, "ef8fda7d-8ef3-4cfc-bbe3-187c4c8d8c8e": {"doc_hash": "4b263b8f4a7d2778ade0193bff675bb6c3888666d40fbfaf3282ecc4f9d348e0", "ref_doc_id": "2c966173-c71d-4a61-8c00-afcb6570a42a"}, "0bc84487-bccd-4395-b278-b1b2f292779c": {"doc_hash": "ea415eec3c3928e3da48ef9aa4639e6ee7024e70e12ca00db82dbefc75d275e2", "ref_doc_id": "2c966173-c71d-4a61-8c00-afcb6570a42a"}, "5bc6cf5f-3375-4439-b922-13ad090950ca": {"doc_hash": "814d5494f1b4fa76c6103c2f8fe8d8e689c46c1ba48f65961374c73bfd1ddb82", "ref_doc_id": "2c966173-c71d-4a61-8c00-afcb6570a42a"}, "01f28555-0d02-4c95-a49c-5e18c154032d": {"doc_hash": "27a42a7f0beb65ed86cff27ba7a567a07c2d2355ac75271281beebf2638d82c3", "ref_doc_id": "2c966173-c71d-4a61-8c00-afcb6570a42a"}, "602e189f-81c5-4eb5-917b-cbcb0fbd7943": {"doc_hash": "b08027cce0b95e1b89adff0d0442513963286d1a14d12ec73dffe20e8bba96d6", "ref_doc_id": "2c966173-c71d-4a61-8c00-afcb6570a42a"}, "4111a776-4110-4df7-a783-5edfee6938d5": {"doc_hash": "dce9839851a1d0c5923b975586dee4ca644501f529a067d3d0089c7c558c6fb1", "ref_doc_id": "341db2a2-ea1d-4906-bdd8-c9a83dceda29"}, "0fc82595-3163-4a2a-8679-414a59cdea05": {"doc_hash": "ba39166aa35ce5ac4694922c5cf7768b7528124aaf1b3ab17512b2997eba9332", "ref_doc_id": "341db2a2-ea1d-4906-bdd8-c9a83dceda29"}, "51ce2bd6-2b47-4c46-b7ee-83a8b0ef44b9": {"doc_hash": "488daa14d07a3213f2332b9d2e2126d500033050a6675df9dadfe2816c62d7af", "ref_doc_id": "341db2a2-ea1d-4906-bdd8-c9a83dceda29"}, "83ac7da0-6cc6-4c6d-a633-d805e2d24be0": {"doc_hash": "c2ccc518cfd3cfa087bcd1dbfe8e13f5ee7f9a06d4e9ff8bd0114ccdfd626fc3", "ref_doc_id": "341db2a2-ea1d-4906-bdd8-c9a83dceda29"}, "f1febf98-47c6-4539-98f4-0802bb771b93": {"doc_hash": "added2637bf47ce4c6595b6a5e8ac13f1f348f7855d89e16d3862f4e1c6957ad", "ref_doc_id": "68a947d1-6c1e-42b1-8ba6-0a3f74cb3a88"}, "46cd61fd-acd1-4c58-8f5c-043b4a8944a1": {"doc_hash": "5c93c87533c401629784bca1baa2ad6969b788666f8c6b8cc711c5fba6a3da21", "ref_doc_id": "68a947d1-6c1e-42b1-8ba6-0a3f74cb3a88"}, "0f00e79f-fe2d-43f7-a8a1-d7ba41a578ce": {"doc_hash": "589da0f2e7c7c04d182ff824e03eece651a95153a7458a2e3994c5e32be09031", "ref_doc_id": "68a947d1-6c1e-42b1-8ba6-0a3f74cb3a88"}, "817c17f0-b69c-4431-92bc-6d0592ed8b2d": {"doc_hash": "de9e882c20ae59bbabc87bffb7176d5d88ab40af52df4c54655374349e9c7bb9", "ref_doc_id": "68a947d1-6c1e-42b1-8ba6-0a3f74cb3a88"}, "ab4867f3-5019-47e8-b6eb-d6c446d8f28c": {"doc_hash": "f3101af0bf6f8007ae3df94f530badc7a38d1e181f4e85ef5606e3814b87bf0a", "ref_doc_id": "68a947d1-6c1e-42b1-8ba6-0a3f74cb3a88"}, "b1500f67-b7c2-4e4e-aa74-6ddd5e568677": {"doc_hash": "b75d4f9466e9955d1a59f8cc3b7a1820cdab51630fe94e3cd266d4f6f28aafc1", "ref_doc_id": "68a947d1-6c1e-42b1-8ba6-0a3f74cb3a88"}, "fb19e0e3-4ab1-4f7e-b25e-08920cfdb93e": {"doc_hash": "73f921a1102b5278ab48fbd35dc92a0050134d4dff808f55ed8a3feccffdf1f5", "ref_doc_id": "68a947d1-6c1e-42b1-8ba6-0a3f74cb3a88"}, "06335322-2d52-4e85-8b4e-e1932f57aa8a": {"doc_hash": "e9a29f958eaa761d83815722e1571888ec3da1b9e3655f67867b9713ebe976bf", "ref_doc_id": "68a947d1-6c1e-42b1-8ba6-0a3f74cb3a88"}, "c13c5070-f277-4a55-aef1-1ba5e14ad10c": {"doc_hash": "222c4aafba881511a857720fe5cc21f2f74068f12483e00da0ae9896c93e070f", "ref_doc_id": "68a947d1-6c1e-42b1-8ba6-0a3f74cb3a88"}, "15ea3d6f-be19-4566-885a-f9acd3a9953a": {"doc_hash": "98a6db48afdd22fbc668c8d92dc5e0419d890c425b50301991c89ae59b691799", "ref_doc_id": "68a947d1-6c1e-42b1-8ba6-0a3f74cb3a88"}, "0c725533-df8f-4e53-9353-9efca6ec2ae7": {"doc_hash": "416960efbffd15290b71ae3328f7821184b95c966a95580c02712555839c7f73", "ref_doc_id": "68a947d1-6c1e-42b1-8ba6-0a3f74cb3a88"}, "7409e5a2-b727-42de-864c-99591a88d124": {"doc_hash": "fbc6268f4b46239cc044445df605206d13de58f1fcb908ce41017320cc6e7863", "ref_doc_id": "d21a27c0-c906-44c8-8ef0-37e5dcf082f6"}, "27cb569c-d400-46aa-8822-e79ad5cb9c3f": {"doc_hash": "d0bc9346fc891ca7e85527f810f3e6e4e96d7c77464fe9fcc4dcede61d9fb05c", "ref_doc_id": "d21a27c0-c906-44c8-8ef0-37e5dcf082f6"}, "5192829f-a132-4383-bf97-6f8da6112994": {"doc_hash": "6c5d90055370714e01b000f296485e0f8fd5d7e441b1bbb50669a4dd6e1ed1c4", "ref_doc_id": "d21a27c0-c906-44c8-8ef0-37e5dcf082f6"}, "6d305b6f-e266-4661-8b27-23c114a1e436": {"doc_hash": "ce28613217c97cee512f3416283048993dd79de1f530d6538b37957874439c03", "ref_doc_id": "d21a27c0-c906-44c8-8ef0-37e5dcf082f6"}, "dc2794e1-23f2-4790-8646-bb201daef271": {"doc_hash": "c8702a50ca2d417f44ee7b7fb90642efb298c630695a43a4f7e05f57ac259f98", "ref_doc_id": "d21a27c0-c906-44c8-8ef0-37e5dcf082f6"}, "3437b056-56d5-4e36-a6e4-fe449f47172c": {"doc_hash": "3ce86f127e4bf1c914920feaac2ad51fb1cbbb52f733c695e827d96a7c20182b", "ref_doc_id": "d21a27c0-c906-44c8-8ef0-37e5dcf082f6"}, "924d7bbb-c072-42cb-bc72-b9cb9bc4e75b": {"doc_hash": "999c2a7e16ad6799ea2cf0afcb6e7e1c09fc1ef087f773dd8e7ea25e0b0033bc", "ref_doc_id": "d21a27c0-c906-44c8-8ef0-37e5dcf082f6"}, "ce6d2624-117c-45c1-811a-5b4c9eca1af8": {"doc_hash": "0e9e1df86cba21ad00fb9b7625e23608b4fad020f9087c824f15d096cecc4ef3", "ref_doc_id": "d21a27c0-c906-44c8-8ef0-37e5dcf082f6"}}, "docstore/ref_doc_info": {"975f80d9-59dd-4926-a5da-0d4af8fa6e7a": {"node_ids": ["1690b49b-e337-4fb2-81ba-6e9d22e0f0d2", "71330889-f270-4edf-84bf-46777db79be0", "fd1de8c7-6e7d-45b0-94dc-926acd2a9931"], "metadata": {"page_label": "1", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}}, "7d9b2c19-f852-466e-b181-07c2b7cb445a": {"node_ids": ["4347790c-3c18-49a5-bb21-dc3b6d1cd3e7", "fe5ab592-b3c9-48b4-826f-85b97b7dac10", "63d7c5e5-5ea1-4d52-b0fa-07f6a29d7ff9", "52a62b94-9292-4ded-9575-a06d26587a42", "98430460-6eed-4a39-a767-90479f1408c3", "0f3b799e-b065-416f-818e-2e44ac00b6aa"], "metadata": {"page_label": "2", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}}, "e78cd701-1c80-4882-82f0-232fb546ed58": {"node_ids": ["cff62621-5066-43e2-85ce-b121d0204585", "903da76a-7bb8-423d-afbc-d54db070cf5d", "c1eea650-23f6-45f2-a27c-68b98e637cb5"], "metadata": {"page_label": "3", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}}, "d9e034f3-f435-456d-9c7d-9a03358f3a73": {"node_ids": ["2a76b8b0-3a7b-4ed0-94f2-4fd315547377", "253e50c5-3d56-4612-96b0-88d48e5492b7", "4faa2f83-321e-41ba-86f3-ea7411c93717", "2a3a7c67-571b-4d7f-8f6c-3538fb7f7a53", "92988799-7642-4005-9b93-d072a5b82633"], "metadata": {"page_label": "4", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}}, "2c966173-c71d-4a61-8c00-afcb6570a42a": {"node_ids": ["0240a1ae-7fd4-49aa-b10d-8edf42ba1e50", "ef8fda7d-8ef3-4cfc-bbe3-187c4c8d8c8e", "0bc84487-bccd-4395-b278-b1b2f292779c", "5bc6cf5f-3375-4439-b922-13ad090950ca", "01f28555-0d02-4c95-a49c-5e18c154032d", "602e189f-81c5-4eb5-917b-cbcb0fbd7943"], "metadata": {"page_label": "5", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}}, "341db2a2-ea1d-4906-bdd8-c9a83dceda29": {"node_ids": ["4111a776-4110-4df7-a783-5edfee6938d5", "0fc82595-3163-4a2a-8679-414a59cdea05", "51ce2bd6-2b47-4c46-b7ee-83a8b0ef44b9", "83ac7da0-6cc6-4c6d-a633-d805e2d24be0"], "metadata": {"page_label": "6", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}}, "68a947d1-6c1e-42b1-8ba6-0a3f74cb3a88": {"node_ids": ["f1febf98-47c6-4539-98f4-0802bb771b93", "46cd61fd-acd1-4c58-8f5c-043b4a8944a1", "0f00e79f-fe2d-43f7-a8a1-d7ba41a578ce", "817c17f0-b69c-4431-92bc-6d0592ed8b2d", "ab4867f3-5019-47e8-b6eb-d6c446d8f28c", "b1500f67-b7c2-4e4e-aa74-6ddd5e568677", "fb19e0e3-4ab1-4f7e-b25e-08920cfdb93e", "06335322-2d52-4e85-8b4e-e1932f57aa8a", "c13c5070-f277-4a55-aef1-1ba5e14ad10c", "15ea3d6f-be19-4566-885a-f9acd3a9953a", "0c725533-df8f-4e53-9353-9efca6ec2ae7"], "metadata": {"page_label": "7", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}}, "d21a27c0-c906-44c8-8ef0-37e5dcf082f6": {"node_ids": ["7409e5a2-b727-42de-864c-99591a88d124", "27cb569c-d400-46aa-8822-e79ad5cb9c3f", "5192829f-a132-4383-bf97-6f8da6112994", "6d305b6f-e266-4661-8b27-23c114a1e436", "dc2794e1-23f2-4790-8646-bb201daef271", "3437b056-56d5-4e36-a6e4-fe449f47172c", "924d7bbb-c072-42cb-bc72-b9cb9bc4e75b", "ce6d2624-117c-45c1-811a-5b4c9eca1af8"], "metadata": {"page_label": "8", "file_name": "1910.02490v3.pdf", "file_path": "data/1910.02490v3.pdf", "file_type": "application/pdf", "file_size": 4412139, "creation_date": "2025-05-12", "last_modified_date": "2025-05-12"}}}}